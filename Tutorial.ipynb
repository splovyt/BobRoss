{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Resurrection of Bob Ross\n",
    "\n",
    "(2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bob Ross was a peaceful painter, famous for his video series on teaching hobbyists how to paint landscapes and beautiful nature in general. Unfortunately, Bob passed away on July 4 1995, leaving his loved paintings behind. However, his legacy continues! In this notebook, I will apply some NLP (Natural Language Processing) and other machine learning techniques to process the transcripts of his video series in an attempt to create a model to guide us with our own paintings.\n",
    "\n",
    "<img src=\"docs/BobRoss2.jpg\" align=\"center\" width=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP processing is a field with a lot of unsolved problems. This notebook is an example that highlights a lot of the problems! \n",
    "\n",
    "\n",
    "If you want to learn more about Bob Ross, you can check out his 28 second long video trailer on Youtube: https://youtu.be/gqdzXNsL_2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies of this script:\n",
    "import pysrt # dealing with .srt files\n",
    "import enchant # libary to check spelling\n",
    "import multiprocessing # Process-based “threading” interface (speeds up training of some of the models)\n",
    "\n",
    "# NLP - Natural Language Processing (Machine Learning #1)\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# NTLK - Natural Language Toolkit\n",
    "from nltk.corpus import stopwords # to remove stopwords\n",
    "# nltk.download('stopwords') # execute this once\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import spatial # distance measure\n",
    "import numpy as np # arrays\n",
    "import math # standard math library\n",
    "import sys # standard system library\n",
    "import time # standard time library\n",
    "import pickle # standard library to save variables\n",
    "\n",
    "# data analysis library\n",
    "import pandas as pd\n",
    "\n",
    "# Multiclass Classification\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading and exploring the data\n",
    "\n",
    "The human-transcribed subtitles were downloaded from https://www.youtube.com/user/BobRossInc/ in .srt format. They are put in the _transcripts_ folder. With 'human-transcribed' I mean transcriptions that were not automatically generated by Youtube. Data of the first 7 seasons were downloaded and used here. One season contains about 13 videos of 30 minutes long.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "transcript_dir = 'transcripts/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load all the transcripts from the transcripts folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 72 available transcripts!\n"
     ]
    }
   ],
   "source": [
    "all_transcripts = [s for s in os.listdir(transcript_dir) if s.endswith('.srt')]\n",
    "print('There are {} available transcripts!'.format(len(all_transcripts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get a basic idea of how a transcript looks like:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Hi, welcome back.\n",
      "I'm glad to see you today.\n",
      "I'm very glad you could join me.\n",
      "Today, I think we'll do a\n",
      "fantastic little painting that\n",
      "I think will make you happy\n",
      "and do good things for you.\n",
      "I'm gonna have them graphically run\n",
      "all the colors across the screen,\n",
      "and they'll come across your\n",
      "screen in the same order\n",
      "that I have them on my palette,\n",
      "starting with the white and going around.\n",
      "And while they're doing\n",
      "that, let's go up here\n",
      "and let's do a fantastic\n",
      "painting together.\n",
      "Gonna start today \n"
     ]
    }
   ],
   "source": [
    "example_transcript = pysrt.open(transcript_dir + all_transcripts[0])\n",
    "full_text = '\\n'.join([line.text for line in example_transcript])\n",
    "print(full_text[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parse the transcripts\n",
    "Parse the transcripts into a list of full and complete sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.85 s, sys: 100 ms, total: 1.95 s\n",
      "Wall time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "def remove_all_brackets(s):\n",
    "    \"\"\"Removes all brackets from the transcripts.\"\"\"\n",
    "    while s.find('(') != -1:\n",
    "        start = s.find( '(' )\n",
    "        end = s.find( ')' )\n",
    "        if start != -1 and end != -1:\n",
    "            s = s[0:start] + s[end+1:]\n",
    "    return s\n",
    "\n",
    "def transcript_to_fluent_text(transcript):\n",
    "    \"\"\"Restructuring and concatenating the transcripts into a more readable format.\"\"\"\n",
    "    sentences = list()\n",
    "    current_sentence = ''\n",
    "    for t in transcript:\n",
    "        for c in t:\n",
    "            current_sentence += c\n",
    "            if c == '.' or c == '!' or c == '?':\n",
    "                current_sentence = remove_all_brackets(current_sentence)\n",
    "                current_sentence = current_sentence.replace('\\n', ' ').replace('-', '')\\\n",
    "                .replace(',', ', ').replace('  ', ' ').strip().replace('<i>','').replace('</i>','')\n",
    "                if current_sentence == '.':\n",
    "                    sentences[-1] = sentences[-1] + current_sentence\n",
    "                else:\n",
    "                    if len(sentences)>1:\n",
    "                        if sentences[-1][-3:] == '...':\n",
    "                            sentences[-1] = sentences[-1] + ' ' + current_sentence\n",
    "                    sentences.append(current_sentence)\n",
    "                current_sentence = ''\n",
    "        current_sentence += ' '\n",
    "    return sentences\n",
    "\n",
    "parsed_transcripts = list()\n",
    "for j in all_transcripts:\n",
    "    t = pysrt.open(transcript_dir + j)\n",
    "    transcript = [line.text for line in t]\n",
    "    parsed_transcripts.append(transcript_to_fluent_text(transcript))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have parsed the transcripts to now contain one full sentence per entry in the list. This is how it looks like for the first five sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hi, I'm glad to see you today.\",\n",
       " 'Glad you could join me.',\n",
       " \"Today I thought we'd do a black canvas And I've taken a black canvas here and I've covered the whole thing with Alizarin crimson and Thalo blue.\",\n",
       " 'And I just sort of mixed them on the brush and just covered the entire canvas.',\n",
       " \"So I'll have 'em graphically run all the colors that you need across your screen.\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_transcripts[-1][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a Mac computer, you can run the following cell to make your computer read the above sentences for you. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "s = 'say \"' + ' '.join(parsed_transcripts[-1][:5]) + '\"'\n",
    "os.system(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will take a look at the words contained in the transcripts and their abundance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract_words(parsed_transcripts):\n",
    "    \"\"\"Extract all words from the parsed transcripts and put them into a dictionary.\"\"\"\n",
    "    words = dict()\n",
    "    for t in parsed_transcripts:\n",
    "        for s in t:\n",
    "            s = s.lower().replace('...', '').replace('.', '').replace('\\n', '')\\\n",
    "            .replace(',', '').replace('!', '').replace('?', '').replace('\"', '')\n",
    "            s = s.split()\n",
    "            for w in s:\n",
    "                if w in words:\n",
    "                    words[w] += 1\n",
    "                else:\n",
    "                    words[w] = 1\n",
    "    return words\n",
    "\n",
    "words = extract_words(parsed_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 most used words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and : 9796\n",
      "a : 8772\n",
      "the : 8136\n",
      "little : 6851\n",
      "just : 5716\n",
      "of : 5110\n",
      "it : 4953\n",
      "you : 4779\n",
      "to : 4675\n",
      "here : 4466\n"
     ]
    }
   ],
   "source": [
    "x = sorted(words.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for (i, j) in x:\n",
    "    print(i,':',j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least used words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodbye : 1\n",
      "curved : 1\n",
      "cage : 1\n",
      "mountain'll : 1\n",
      "desperately : 1\n",
      "dines : 1\n",
      "tends : 1\n",
      "da : 1\n",
      "starburst : 1\n",
      "putty : 1\n"
     ]
    }
   ],
   "source": [
    "x = sorted(words.items(), key=lambda x: x[1], reverse=True)[-10:]\n",
    "for (i, j) in x:\n",
    "    print(i,':',j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spell check the words, since the subtitles were made manually.**\n",
    "\n",
    "Ideally, these spelling mistakes should be evaluated, but we will leave it like this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looksee\tdibbydoo\tfenugreek\tputtin'\tusin'\tunderpaint\tpurty\twater'll\tcopal\tgreatgrandfather\tjaldi\tlowling\tgrey\tfrance\tbrushstrokes\tonethird\tmumbai\tlefthanded\tvandyke\ttreeit\tmonday\tdoop\tdoo\tamerican\tboop\tok\tstringies\tpression\tune\ttwostar\tsaumon\trockytype\tespagnole\tbeaucoup\tchewin'\tgoin'\tsockin'\tcepe\tcolors:\tchalo\tkala\tou\tnobody'd\thassan\t'im\trigght\tlettin'\tmurgh\tunderpainting\tfillin\tphiltrum\tbonsoir\tsouffle\tbarbeque\ttwoandahalf\tpennisula\theighth\tbritish\tdropoff\tnothin's\teffect;\tphthalo\tdifferesco\twater'd\tmukhtar\t1/2\teverevident\toooo\tprotuding\tonehaired\tthelo\tiqbal\tsaturday\talaskan\tlayin'\tbaleine\tnothing's\tcrossways\tinchbrush\tm'kay\tn'estce\tindia\tdoodoodoodoodoo\tthat''s\ti've\t'bout\t'em\tlookin'\tnitro\tmasala\ttim\twmvt\tmarseillaise\tbein'\tpayne's\tlivin'\tkadam\tlets'\tnah\tbellowy\tunpure\thayinfused\tle\tbrushmixed\thappenin'\thaan\tthierry\tdododododododo\tdamar\t'ole\teverything's\tworld:\tmmm\talaska's\tshoom\thave's\tmistakes;\tchristmas\tbastille\thighlight'll\txes\tboletus\tkay\tfeuilleté\tindian\tjeanpierre\thzooop\tyay\t\n"
     ]
    }
   ],
   "source": [
    "def not_recognised_words(words):\n",
    "    \"\"\"Return potentially misspelled words from a list, using the 'enchant' en_US library.\"\"\"\n",
    "    not_recognised = list()\n",
    "    dictionary = enchant.Dict(\"en_US\")\n",
    "    for w in words:\n",
    "        if dictionary.check(w) == False:\n",
    "            not_recognised.append(w)\n",
    "    x = ''\n",
    "    for i in not_recognised:\n",
    "        x += i + '\\t'\n",
    "    return x\n",
    "\n",
    "print(not_recognised_words(words.keys())[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Cluster sentences (unsupervised learning)\n",
    "\n",
    "Next, we want to group similar sentences in clusters. This will enable us to find a pattern in the transcripts that explains when one cluster of sentences is used by mister Ross. \n",
    "\n",
    "<img src=\"docs/goal_of_clustering.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Calculate the similarity between two sentences\n",
    "\n",
    "In order to accomplish the above, we need to go into the field of NLP (Natural Language Processing) to make the computer understand the content of the sentences in order to score their similarity. If we know what sentences are very similar, we know which ones to put together in the same cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity scoring can be done by using Word2Vec. Word2vec takes as its input a large piece of text and produces a vector space, typically of several hundred dimensions, with each unique word in the piece of text being assigned a corresponding vector in the space. This sounds complicated, and it is, but the essence is that the model mathematically represents the words based on the surrounding words and sentences (= it's meaning derived through context). We are essentially teaching Word2Vec what each word means and where it is used (applied Machine Learning). To teach well, we need to input a large corpus of text examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What better way to teach Word2Vec the 'meaning' of words than by using the entire English Wikipedia database, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1.1 Training Word2Vec on Wikipedia\n",
    "\n",
    "First, you should download all English Wikipedia articles. You can do this as follows:\n",
    "\n",
    "- Find the code for the language you want, in this case English, which is represented by 'en'. In this case the url to the dumps is https://dumps.wikimedia.org/enwiki/latest/. We find this url by adding the language code 'en' before 'wiki' and the end of the first part of the link.\n",
    "  \n",
    "  \n",
    "- Look for the file named enwiki-latest-pages-articles.xml.bz2 and download this (huge file!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "file_location = 'word2vec/enwiki-latest-pages-articles.xml.bz2' # change the filepath to the downloaded file here\n",
    "\n",
    "def train_word2vec(path_to_data):\n",
    "    \"\"\"Train a Word2Vec model on Wikipedia Data. Warning: This takes 10+ hours on average!\"\"\"\n",
    "    wiki = WikiCorpus(path_to_data, \n",
    "                      lemmatize=False, dictionary={})\n",
    "    sentences = list(wiki.get_texts())\n",
    "    # size is the number of features, this will be important later\n",
    "    params = {'size': 300, 'window': 10, 'min_count': 10, \n",
    "              'workers': max(1, multiprocessing.cpu_count() - 1), 'sample': 1E-3,}\n",
    "    word2vec = Word2Vec(sentences, **params)\n",
    "    return word2vec\n",
    "\n",
    "# refs: \n",
    "# - https://github.com/hgrif/wiki-word2vec\n",
    "# - https://github.com/panyang/Wikipedia_Word2vec\n",
    "# - http://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to train: (WARNING: can take 10+ hours and uses a lot of computer recourses!)\n",
    "\n",
    "To run the code, simply click the next block and change the type of the block from 'Raw NBConvert' to 'code' and press run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = train_word2vec(file_location)\n",
    "\n",
    "# Save the model under a name to use later (change this to the desired output file path)\n",
    "output_file = 'word2vec/wiki.en.text.w2vbinary' \n",
    "word2vec_model.wv.save_word2vec_format(output_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing the above once, your Word2Vec model is ready for use. You reload it from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 s, sys: 4.03 s, total: 24.3 s\n",
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "input_file = 'word2vec/en.wiki.text.w2vbinary' # = output_file from the last block\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(input_file, binary=True)\n",
    "\n",
    "# index2word\n",
    "index2word = set(word2vec_model.index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1.2 Testing the trained Word2Vec model\n",
    "\n",
    "As stated before, Word2Vec creates a vector space in order to represent the context (meaning) of the words. Therefore, to understand the meaning of a combination of words, we simply need to combine the vector representation. Again, this sounds complicated, but the following image will explain a lot:\n",
    "\n",
    "(ref Tensorflow - https://www.tensorflow.org/tutorials/word2vec - image source and more information)\n",
    "<img src=\"docs/word2vec_vectorspace_relationships.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be interpreted as follows: \n",
    "\n",
    "\"a man is to a woman what a king is to a queen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put Word2Vec to the test. If we pass 'woman king' to Word2Vec and put 'woman' in contrast with 'man', we should be able to find 'queen' as result. See for yourself, given the top three results:\n",
    "\n",
    "\"Woman + King == Man + ???\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6773032546043396),\n",
       " ('princess', 0.5975905060768127),\n",
       " ('monarch', 0.5914919376373291)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(positive=['woman', 'king'], negative=['man'])[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example, also related to politics:\n",
    "\n",
    "\"Trump + Republican == Obama + ???\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('democrat', 0.6498938798904419),\n",
       " ('democratic', 0.5777391195297241),\n",
       " ('gop', 0.5344028472900391)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(positive=['trump', 'republican'], negative=['obama'])[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1.3 Applying the trained Word2Vec model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the trained Word2Vec model to compute similarities between Bob Ross his sentences. The most similar sentences will be clustered together. We will create the pairwise_similarity function that has as inputs two sentences and as output a score. Generally, the cosine similarity (similarity = 1 - distance) tends to perform best.\n",
    "\n",
    "First, we use Word2Vec to convert the words to their representation in the vector space. Secondly, we average these representations over the sentences. We then compute the cosine similarity between two average representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_feature_vector(words, model, num_features, index2word_set):\n",
    "    \"\"\"Compute the average vector of the combination of words.\"\"\"\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    if(nwords>0):\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "    \n",
    "def pairwise_similarity(sentence_1, sentence_2, num_features=300):\n",
    "    \"\"\"Compute the pairwise cosine distance between two vectors.\"\"\"\n",
    "    # get average vectors for both sentences\n",
    "    sentence_1_avg_vector = avg_feature_vector(sentence_1.split(), model=word2vec_model, num_features=num_features, index2word_set=index2word)\n",
    "    sentence_2_avg_vector = avg_feature_vector(sentence_2.split(), model=word2vec_model, num_features=num_features, index2word_set=index2word)\n",
    "    \n",
    "    # calculate similarity with spatial.distance.cosine\n",
    "    sen1_sen2_similarity =  1 - spatial.distance.cosine(sentence_1_avg_vector,sentence_2_avg_vector)\n",
    "    return sen1_sen2_similarity\n",
    "\n",
    "# ref: \n",
    "# https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1.4 Performance Test \n",
    "\n",
    "We will compare one sentence with two other sentences. One sentence that is not very similar (Example 1) and another sentence that is more similar (Example 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: 0.255127590975\n",
      "Example 2: 0.540638658672\n"
     ]
    }
   ],
   "source": [
    "main_sentence = \"And this is much different than the traditional palette knife.\"\n",
    "\n",
    "# 1\n",
    "sentence_1 = \"Be sure you don't start with a nylon brush.\"\n",
    "print('Example 1:', pairwise_similarity(main_sentence, sentence_1, num_features=300))\n",
    "\n",
    "# 2\n",
    "sentence_2 = \"I have a traditional palette knife in my back pocket.\"\n",
    "print('Example 2:',pairwise_similarity(main_sentence, sentence_2, num_features=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the score of the second, more similar sentence is 2 times as high as the score of the less similar sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Clustering Bob Ross his sentences together in different groups (classes) \n",
    "\n",
    "To cluster these sentences, I will go for the more intuitive approach. I don't claim that this is the best method, but it's probably one of easiest to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will iteratively group the most similar sentences until the specified amount of clusters is achieved or the level of dissimilarity between clusters (or similarity within clusters) is achieved. This approach is called an ** agglomerative bottom-up approach**.\n",
    "\n",
    "In practice, we start with all sentences in different clusters and we then iteratively select the two most similar sentences and group them in just one cluster (merging). We then look for the second most similar combination and add it together and so forth until the stopping criterium is satisfied. The stopping criterium will be a minimum within-cluster similarity score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"docs/clustering_illustration.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.1 Clustering step 1: Get all unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 20862 unique sentences.\n"
     ]
    }
   ],
   "source": [
    "unique_sentences = set()\n",
    "for transcr in parsed_transcripts:\n",
    "    for sentence in transcr:\n",
    "        sentence = sentence.replace('\"', '')\n",
    "        unique_sentences.update([sentence.strip()])\n",
    "print('We have',len(unique_sentences), 'unique sentences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.2 Clustering step 2: Assign a unique ID to the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('s1', \"'Cause I want this mountain to be in front.\"), ('s2', \"'Cause at the bottom of the mountain we have mist, and now we have pollution.\"), ('s3', \"'Cause clouds are free.\"), ('s4', \"'Cause dark colors are needed to make the light color show.\"), ('s5', \"'Cause everyone sees nature differently.\")]\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "all_sentences = []\n",
    "for s in sorted(list(unique_sentences)):\n",
    "    all_sentences.append(('s'+str(i), s))\n",
    "    i+=1\n",
    "print(all_sentences[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.3 Clustering step 3: Simplify the sentences\n",
    "\n",
    "- removing short non-informative words like 'and', 'or', 'the', 'it', 'we',...\n",
    "\n",
    "- removing punctuation marks\n",
    "\n",
    "- putting everything in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20734 sentences left\n",
      "128 sentences removed\n"
     ]
    }
   ],
   "source": [
    "punctuation = ['.', ',', '!', '?', ':', '\"', ';', '/', '&', '(', ')', '[', ']']\n",
    "other_words_to_remove = ['okay']\n",
    "\n",
    "def remove_stop_words(text, other_words_to_remove=[]):\n",
    "    \"\"\"Remove stop words from a piece of text.\"\"\"\n",
    "    s=set(stopwords.words('english'))\n",
    "    for w in other_words_to_remove:\n",
    "        s.update([w])\n",
    "    return ' '.join(filter(lambda w: not w in s,text.split()))\n",
    "\n",
    "for n,(id,sentence) in enumerate(all_sentences.copy()):\n",
    "    # lowercase\n",
    "    simple_sentence = sentence.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    for c in punctuation:\n",
    "        simple_sentence = simple_sentence.replace(c, '')\n",
    "        \n",
    "    # remove stopwords with nltk\n",
    "    simple_sentence = remove_stop_words(simple_sentence, other_words_to_remove)\n",
    "    \n",
    "    all_sentences[n] = (id, sentence, simple_sentence)\n",
    "\n",
    "# make sure every simple sentence in all_sentences is valid:\n",
    "removed_count = 0\n",
    "for (id1, s1, ss1) in all_sentences.copy():\n",
    "    if ss1.lower() == ss1.upper():\n",
    "        removed_count += 1\n",
    "        all_sentences.remove((id1, s1, ss1))\n",
    "print(len(all_sentences), 'sentences left')\n",
    "print(removed_count, 'sentences removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"docs/sentence_illustration.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the end result with the simplified sentences look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s111', 'A little bit more of the paintthinner.', 'little bit paintthinner')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentences[109]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element is the sentence ID, the second the original sentence and the third is the simplified sentence. We will do the clustering on the simplified sentences, because these contain the 'essence' of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.4 Clustering step 4: Calculating pairwise similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to start clustering, we will start by computing all pairwise similarities between the simplified sentences (the core of the sentences) and add these in a list. To compute the distances (1-similarity), we will use the pairwise scores on word2vec vectors, poured into the pairwise_similarity function mentioned before.\n",
    "\n",
    "Because a lot of distances need to be calculated and the process below is not efficient (quadratic in the amount of sentences; nested for loop), this process takes a very long time. The clustering itself will be done using efficient disjoint sets, but the process below needs to be optimized for future projects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict how much time it will take for the similarity calculations to run, I have collected the time required for a certain amount of sentences. With this data, a regression model of degree 2 was fitted. We will use this model to predict how long it would take the process to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equation = 4e-05x^2 + 0.01x + -2.16\n",
      "R^2 = 0.9997\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGcCAYAAAD+oCs8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzs3XeYVNX9x/H3R+wNibEbjQoajTERTNQYUSOiYo0tLhFb\n7A1RY28xP41dklijsaJrIRoVEBRUxBIL2BtFUaOCIAiCFfb8/jhn9DLM7s4Ouzszu5/X88yzO/ee\nufc7d+bOfOe0qxACZmZmZtVgoXIHYGZmZlYsJy5mZmZWNZy4mJmZWdVw4mJmZmZVw4mLmZmZVQ0n\nLmZmZlY1nLiYmZlZ1XDiYmZmZlXDiYuZmZlVDScu7YSkcyXVNfM2t5JUJ6l7c263hDgOSHF0LaLs\n45Iea8Z9n5v2/YPm2mZ7JmmipAfKHUc5SLpZ0ufljqNYknaQ9KKkLyXNlbRsuWOy9sGJS4XJfAnn\nbt9K+p+kmyStugCbDulWSkxHSjqgge1WgmLjCEBzJnAlH9f6SKqR1Lc5t1lJJK0v6RxJaxRYXSnv\np3Jo9vdSS0mJ+l3AF8BRQB9gdlmDyiPpNEm7lTsOa34LlzsAKygAZwETgcWBzYCDgC0kbRhC+KaV\n4zkKmALcMk+QIYyUtEQZ4lkQ25U7gCL0Bn4K/K3cgbSQDYBzgMeA98sci5Xml8DSwJkhhGarwWxm\npwP3APeXOxBrXk5cKtfQEMKY9P+Nkj4FTgZ2BQaWL6x5VVnSQghhTrljMESV1Cy0RZIWDyF8tYCb\nWSn9nbGg8Zg1lZuKqsco4gf+OvkrJO0o6QlJsyTNlDRI0gaNbVDSQZJGSJos6StJr0s6Iq/Mu8Rf\n/1tnmq8eTesK9nGRtLekFyR9IWmKpNvym7ly7fmSVpX0n/T/J5IukaS8svum7c2UNEPSK5KOK/CU\nFpN0edrOLEn3Slo+b1uP5+LPew77SLpA0sfpsfdLWr2xY5ixgqS7U3xTJfWXtFh+IUn7ZY7Np5Jq\ns/tJ/W92AtbMHO930ropki7NlJWkz1Jz4rKZ5aekZUtmlq0naWDa55eSnpe0S4H4OqbY30/viXGS\nTs6+JpJysZ0g6VBJ41PZ5yRt0tBBSk2Od6e7j6ftzC3wHtpC0rMp1gmS+pQSawNxTJT0QGP7UT19\nwyQdmGJfo8A2t0rH94v0Xt0qrd8j3f8yvQd+UU9sa0kalt6HH0o6q0AZSTpe0mtpe5MkXStpuXqe\nZ88U01fAYY0cmwbP3/QevTndfSEdhxsb2N7S6XV6N71OkyU9nP/8JW0qaWh6T89WPFd/nVcm16ds\nHcXPkOmp/I2SFs+UqwOWBHKv0zwxKn7u3JiO21fpOB6ct6/cZ8Peks6Q9EE61sMlFfoc3lTSEEnT\n0mv3svI+p1TEeShpYcWm1LGpzFRJoyRtW98xbndCCL5V0A04AJgLdM1bfjSxb8Zhecv7pPKDiU06\nJwETgE+BNTLlzgHm5j32WeBfwHHpsQ+lfRyZKbMrsTr/daCG2IyxbVq3Vdp390z5A9M2nknbPZ/Y\n9j0BWDZT7qa0/FXgeuKH6d1pe4dnym2XtjcMOCLd/gbU5h2zOmA08Eh6LhcD32bLpbKPAY9m7m+V\nHvsy8CLQN8X8BfAmsFgjr9c5mcf/BziS2KRWB9ycV/aM9PxuBw4HzgQ+yR4bYFtgDDA5c7x3Tev+\nAzyX2d7P036+BXbMLH8QeDZz/6fA9HSsT0oxPpZi2S1Tbon0PKYA5wGHptdpLnB5ptyameP9dtrm\niem5vAd0aOB4/Rjon7Z5Xnp+vYEV0vp303H/CPhLivV5YA6wflNjbSCOYvcz33mTd56uUWCb/yM2\n9R4HfECsleid1p8E/Cm9Hm/nbfOm9L57m5gYHEls5qgDzs0rez3wNXBNeu4XAJ8D/80e/7TPscBU\n4vv6UDLna4HndSCNnL/E9+i16fmfnp7bpg1s83bgS+I5eVA6BvcBNZkyvwW+Ap4Ejk/7fjEt26TA\n+Taa2Ax0OHBdiuWvmXK90z4fz7zHNk3rVkyvy8QU/2EpnjrguAKfDaOB51JMZwGzgGfynuN2KdZ3\ngLPTNq8AhpVwHp6fll0DHJyOxwDgT83xHdMWbmUPwLe8F+T7D8RtgOWB1YA9iV9ks4FVM2WXAqYB\n1+RtY4V0glybWVYocZnvS5mYvIzLW/YqmS/7zPJ5Ehdi0+Mk4CVg0Uy5XukD4JzMstyXzOl52xzN\nvF/OVwDTijhmdcTmtezyy4BvgGUyy+pLXN4Hlsws3ystP6aRfec+SO/NW35len4bpvtrEBOMU/LK\nbZBiPDWz7EHgnQL7OjGVXSrdPyZ9UD4DXJApNw24NHN/OPFLYOG87T0JvJW5fyYwE1g7r9wFab+r\npfu5xOUT5k1Gd0nPuVcjx2xP8hLezLp307pfZ5b9kPTF19RYG4ih2P00NXGZS+ZLnO8T71nA6pnl\nh+YfA74/J67I29eDKa4fpPu/Sdv8fV653L72LRBTj4aORwnnb+75dy1iu9OBvzdS5m1gcN6yxYgJ\n09DMstz59s+8sv8GPslb9jlwY4F93UBMLpfLW34H8dxZLN3PfTa8xrzJ4LHpuW+Q7i9EPA8nkPms\nKbDfYs/DF4EHGjuu7fnmpqLKJGAE8dfkB8RfFrOIv7w/ypTbDugI3Clp+dyN2H/gWWLyU68Qwtff\n7VBaNj32CWBtScuUEPcmxF8zV4dM35cQwhDgLWITSL7r8u6PAtbO3P8MWFrS9o3sOwD/LLCtDsQv\n2sbcEkL4IhPzQOBj4od2YwJwVd6yfxBfx9zj90z378l7rT4BxtHIa5WMIn655KrPt0zLRqX/kbQR\nsFxahqROadv3AB3z9v0w0EXSKml7e6XHzcgrNyLtN3/Y+50hhJl58Yl5X79SvBFCeDp3J4QwlfjF\nlt1uU2MtdT+lxP5s5n7u/xEhhP/lLa/vWOW/l64kfon3SPf3Jp4XI/Ke+4vEz4n899K7IYThRcRe\nyvlbjM+AX2XeZ/NITUZdgNq857MM8fXMfy0DhT83lpe0dBHx7EFMBjsUOB86AvnTKtwYQpibt6/s\na7cxqSYxhFBwOHsTz8PPgJ9K6lzEc2mX3Dm3MgVic8c44ol0MPHkze8I24V4Aj1WzzZmFlj+HUlb\nAH8mjlpaMrMqpP02dU6JNdNjxxZY9xawRd6yr0IIn+Ytmw50yty/mvhBPUTSR8ST/O4QwrAC+/ig\nwLbI2159xtezrJikp9DjxxN/reUe35n4y6zQfgLzv7aFjCE2JWxJbBL7DfEX6GTgWEmLpnWB+Csu\nt18Rm0P+r559r0hM0roAPyMmzPWVy5rneIcQPlPsXlLM8W5IoZFG+e+LpsZa6n6aap5thhBmpmPy\nv7xyuU6t+fuqI/56z8qdT9n30nLEpDdfoef+bsMhf6ep52+xTiY2fX0gaTQwBLg1hJCLq0v6e2s9\nj6+T1DGEkO0InP/aZc/1WfUFImkF4rE7jNjMlK/R9znzf66skx73en37pWnn4dnEZuGxkl4j1oIP\nCCG82sD22xUnLpXr+ZBGFUm6n/hFdIek9TI1AwsR3/D7Eb+88tU7gkbS2sSqyzeBfsST8xvir6rj\nKa3jdqOdIvPMbaxACGFK+kW2PbBjuh0k6ZYQwkFFbq+pcS3o4wpZiPiltAOF55Gp98M2J4QwR9Kz\nQPfUOXAVYg3ZFGARYFNiMvNmJiHMvY6XEvsJFTI+U/YR4CIKP/f8L7TmPt5N2W5TYy11P6GeMh2a\nuM0FOVb5ZRYinu+963l8fjL3ZRH7KDaWJgsh3CPpCeB3QE9i/45TJP0u/QDJvUdPJPZbKiT//Cj1\neOb2NYC86R0yXmnivoo5bkWfhyGEUen83o14vA4BTpB0eAih3k7Q7YkTlyoQQqiTdBqxZuUYYic3\niG2qAqaEEB6t7/H12AVYFNglhPBhbmE9Pdfr+/DONzHFsx6xU1zWesSOm00W4hDmwemGpGuAwyT9\nJYSQ/+u0VF0KLFuH+j9ICz0++/xyNSwT0/3cazUxhFCo1iWroeM9ivgLdjvi6z4WQNLrxFq5LYnV\n4Dm54/NtEe+RCcDSoeXn5Sj2/dSQ1op1OsSm1LxmsR+30P4WIjZBZN8j66a/E9PfCcQOsk9nm3ub\nwURa4PwFCCFMJnbovVbSD4nNWmcQv8QnpGKfl/A51uBuCyybQqxJ7tCM+xpPPG4bAvVtsynnISGE\nz4iJ1S2KowNHAecCTlzwcOiqEUIYSezZfnxqEoB40s8ETpc0XxKaPiDqk/sV8d17QFJH4qiCfLOJ\n1auNeYFYfX2EpEUy290RWB8YVMQ25qHCU+nnqkznG268APbPto9L2ptYozGkiMeKOOor6zjiB+fQ\ndP9eUgfHghuY93nOJjbVFTKKOClhX75vDiL93yfFPCq3MIQwhfgldLiklQvsN/seuRvYXFLPAuU6\nSqqvlqGpZhOPWTHvqfq0Vqy5hPO7fhaSlgL2b6btF3JMgfvf8P2X4t3EH51n5z9QUod0HpeiJc7f\nhZR3KYDUl+gjvj9/RxOP80np2OZvo6HPsYbM97kVQqgjduTdU9JPm2lfY4jNccfXd+ybch7mf+al\nGvbxNO/nXVVzjUtlqq/q8RJi564Dib3qP5d0JLFteIykO4m/KNYgNvk8SfwCLeRh4iiXQZKuI3aE\nO4RYBZ1/Yo0mfpidQTyBPsn80v0u1tSUcQrxV8ETkmrTto4j/uLoX9zTn8cN6UR+lNhP4MfED/KX\nQghvZsrVd8yKrf6eBjwp6aYUc19ic8MNRT5+rdSkNxTYnNh89127dAjhHUlnAhdIWovYhv058df1\n7sTOhpenbY0G9pF0GXGI7qwQQu5L4xliE+C6zNtB8Qni8MpAJnFJjk7LXpV0PfG1WCnFuRqxcyHE\n99euxPfEzSmOpYCNiB0af5yO04J6iZg4n6I478jXxM6rU5uwjdaK9WFif4obJV1CTD4PIn7B/6gZ\ntp/va2AHSbcQhzb3IjaPnp9r/gshPJHO2VNTM2ruXF6X2Gn5OGKi3CQlnL/FnFvLAP+TNJBYezmL\nWFu4CXBC2m+QdAjxR8Lr6Rz8kPje3IbYH6iUqftHAz0k9SMmSu+GEJ4DTgW2Bp5N58MbwA+AbsRh\n2U1KXlL8RxGHrr+U4v8Y+Alx5NGOqWix5+Ebkh5P8U8jzlK8F/D3ph6ANqvcw5p8m/dGA8MMiR8U\nY9NNmeXdiSf9NOKvjLHE+Vk2zpQ5B5iTt72diFW2uXkaTiQmRfnDPFcEHiD2dp9LGk5MgXlc0vK9\niL/evuD7SwWsklfmJmBGgec4T5zEdvGHiB8EXxJ/2VwFrNjYMSsUH7G5bUSBMvsQO819TPxwvZ/M\n8NUGXq9ziInEesRfwp8R58zoT2ZIaab87sBIYk3ZTGKHvr8BnTNllgRuI87FM5e8odHEESlzmHd+\ni1VT2XfrifPH6Zh/SJxv4v30HHfPK7dkOg5vp+M9mfhhezxpSCixE+dcoF+B/cwFziriuB1M7Hz+\nTfY1Sq/v/QXKz/O6FRtrA/t/pwn7+QXwdOb9dxyFh0PXt825wN/yls13DNPrMyO9VkOJie1H9R1P\n4I/EWthZ6X33EnE4+EqNxdTIsSnm/C1qODSx79WFxFqJz9J7fgx581GlshsRf5h9ko71O0AtsHXe\n+TaXNDS8QDzZ12Pd9HrOSutuzKz7ITERmEg8Hz4kJoAHF/hs2KOe127/vOWbp9ct9zxfJDMnViPn\n4e8yZU4j/kD5NMX+OnBKY+/p9nRTOlBm7ZLirKaPAXuFEJr8K9XMzFqX+7iYmZlZ1XDiYmZmZlXD\niYuZr1RsZlY13MfFzMzMqoZrXMzMzKxqOHExMzOzquHExdokSTdLKvbicoUef4CkOkn5V4ptFySt\nmZ5/S84Qm93fuZIKXcPJmlF6TeebcbecKjEmq2xOXKytChTR6VbSkZIOaGAb7VlrPv+iXq+2TtIq\nks6RtNECbGNHSQUvLUGZjnMlxmTVy1P+W3t3FN/PDmpJCOE9SUsQp5K31rMqcXbYd5n/KsXF6kV8\nX/+5wLolaOCq8S2oEmOyKuUaF7MKk64GW3YhhG9CI8MOKyXWNqTYa2uVtI30mpajSa4SY7Iq5cTF\nWo2kRSSdJ+kFSZ9JmiXpCUlb55XL9a84QdKhksZL+krSc5I2KbDd3SW9JulLSa9I2r3IeN4Ffgps\nnfZXJyn/kvOLSbpc0icp3nslLV9gWzum5zJL0kxJgyRtUEQMub403SVdLWky8EFm/aqSbpQ0KR2D\n1yQdXGA7q0n6T9r/5BRzz9y2M+UmSrqxwOMfzz73Qn1cUr+hzyWtLWmIpJnAgMz6TSUNTa/t7LTN\nXxfY128kPZ9er3GSDmvsOOU9ftO0/2np+b4s6bi8Mr+VNCqtn56OzU/yypybnmMXSQNS3J9IOi+t\n/1F63AxJH0s6Ie/xW6XH7yPpglRmlqT7Ja2eV7bR4654+YnniM0mN6dtz829Bum43SXpvfReeD+9\nzotntncTsWYj13ekTtLczPr5+pNI2ljSQ+l5fi5puKRN88rk3qe/LuZ8yHtsk2Ja0NcllVlU0p/T\n+yt3rC6StGhDsVp1cFORtaZliRfXqwX+Sbxy7B+BoZJ+FULIrxr/A7A0cC3xw/wU4N+S1g4hzAWQ\n1BMYCLxGvOrr8sSLmP2viHj6AlcSL2b3f8RfhZMz65XWTwPOJV4grV9aVvNdIakPcDPxAmsnEy/+\ndyQwStLGIYT3i4jlauLF5f5MvMoxklYkXlBxLvGCcFOJVwq+QdLSIYS/p3KLE6+evTrxgo0fA32I\nV7rNrzGprwalmD4GgfiZMYx4McMTiRfiQ9JviRf6fIF4rHJXUX5U0m9CCC+kchumx38CnE28CN+5\n6X6jJG0HPEi8+GB/YBKwPvGCobnj0SPFMoHY7LIE8cKIT0rqmnk9cs/5LuIVgk9J2zlD0jTgcGBE\nWt4buETScyGEJ/PCOiM93wuJFyTtBzwi6RchhK/z9pUvu/zNdEzOI175O3eV76fT372J762riRfg\n+xVwLPHKwr9PZa4lNjf1IJ4/DdbgKCbXTxAv7nghscnmcOBxSd1DCM/nPeQfNHI+FNCkmFjA10WS\niO+RXxOP41vAz1KsXYhXD7dqVu6rPPrWfm7ED6yF85YtS/yivT6zbE3iF8EnwLKZ5bsQv8R7ZZa9\nSExSls4s2zY9/p0iYnqVdLXrvOUHpG0MzVt+GfGKxsuk+0sRP8ivySu3AjAduLaR/ef28ziZK36n\ndTek57Zc3vI70j4XS/f7kncVW2Bx4lXC86+O/S6Zq+Rmlj+WPQ6Z12D/zLKb0vb+r8Dj3wYG5y1b\njJg8DM0su494NfLVMsvWI/almdvIsVqIeMXgCbnjX0+5F9N7qmNm2c+IX8o3ZZadk57j1Xn7eD+V\nPTGzvGOKO3uF4a3S498Hlsws3ystP6aE494t/7hnj2eBZaekWFfPLPtHfccybfvsvNfjS2DNzLKV\niYnMY009Hxp4TZoS04K+Lvul99Pmefs5LL1/N2soVt8q/+amIms1IZoD8VeRpE7AosRf6YWGHd8Z\nQpiZuT+KmPysnbaxMvBz4OYQwqzMfkYQf6ktcMjEmqGsUUAH4hc7QE/ih+edkpbP3dJjnwW2KXI/\n14f06ZqxB/GXY4e8bT8MLMf3x2xH4OOQubp1COGrArE3l2uzdyT9gvhLtjYvzmWIv4y7p3ILAdsB\n/wkhfJiJ9W1iLUxjNib+yu8fQvi8UIHMe+KmEMKMzD5eBR4hdhLNCsC/MuXqiO9HERO13PIZxORs\n7QK7vSWE8EWm7EBi4pS/rwUSvq+9QdKS6Rg/Q/xS37ip28u8HveFEN7L7GcSMTneUtLS2RBo/Hxo\nLgvyuuxFrL0am/d+fCw9vphz0iqYm4qsVSkOPT4B+AmxmSDnnQLFP8jeCSF8FmuB6ZQW5T4sxxd4\n7NuU8GHeWAzEWpRsDJ2JH4aPFXhsIP5yLcbE7B1JKxCTk8OIVeOFtr1i+n9N6j8GzW1OCCG/Ga5L\n+ntrPY+pk9SRWAu0JDCuQJm3iQlYQ9YhPu/XGyiTe0+MLbDuTaCnpCVCCF9mluc35c0AvgohTCuw\n/AcFtlvo2I+nmb/MJf0I+Aux5rFTZlUgJs9NtQLx9ajvWAn4Ufo/p7HzoTmV+rp0IX6+TCmwzex5\nY1XKiYu1Gkn7EX8t3QtcTGwKmgucTuFfsnMLLIPv28hzfwv1H2iO0Rn1xaDM9hdK+9+PefvH5BQ7\nzPPLvPu52tAB1D9UO9cnSBR/DOrra9GB4mL9usCyXKwnAi/X87hZxL4m9cVQzOvVXGXyFXqNG3vv\nNTWOBTruqXZkODGZ/Ssx0ZtN7N9yC6UNtGiuY1XqtkrZVzH7X4jYBNyPwnHlJ19WZZy4WGvaE5gQ\nQtgruzA3WqAEE9PfdQusK7SskFImvso+ZgLxw3FKCCF/RNKCmELsNNyhiO1OBDYssHy9AsumE7/8\n8q1JfC6lyD3u84ZilfQJMUEr9NoUijXfeOKx3pDYGbmQiQ1s7yfA1LzalubQpcCydZg3iSv2uNf3\nfvxZ2k+fEMLtuYWpI3K+Yt/TnxA7Vxc6Vuun7TTXl3xrTjA3AdgohFCoFtTaAPdxsdY0l7wPsDTs\ncvNSNpba4l8CDpC0TGab2wGNDkVOZlP4C6VYw4CZwOmS5vshIOmHpWw0ten/G9hT0k8b2e4QYBVJ\ne2bWLwkcWmDTE4DNsrFK2oXYJFCq0Wm7J0laqr5Y03MaBuyuzHBhSesT+wo1Zgyxk+vxqelpPnnv\niWUz+9gw7WNwsU+qCfbP9gWRtDewCvF1ySn2uM9Of/Pfk7mahvzP7OOZPymYnba/LA1Ir8fDwG6S\n1sjEtRJxlNAT2b5jC6iomJrJ3cDqkuZ7/0taXJ57qOq5xsVa0yBgD0n/IX6BrE3sv/E6cdhzKU5L\n231KcZ6M5YFjiMOji9nmaOAISWcQf9F/kvmlVl/193fLQwifSzqS2L9jjKQ7ibUlaxCHcD5JHIrb\nkPr2cyqwNfCspOuJHY5/QBx58lsgl7xcT3zOtynOc5MbDj07f4PEkUp7AcMk3U2sGdiPwv00ihJC\nCJIOIX5Rv57m7fiQ2IyxDbEPwm6p+DnADsShyVcT+zkdQ3wP/KyI/RwF3A+8lPbzMbEmZYMQQq6P\nzJ9SLP+V9C9iP45jiLUehWZuXVDT0vO5iTgipy+x38gNmTLFHvcJwGfE9+Qs4mv4X+KQ3gnAZSnp\nm0mswSyUdI8mvqf+IWkYcTTPXfXEfiZxmPJT6fWYS+xXtShxaH9Wo+dDA5oS04K6DdgHuEbSNsBT\nxCa59YlDynsSk2CrVuUe1uRb+7oRh2++Q6yifoHYIfMmYhNSrsyaxA/QfgUePxc4K2/Z7sRE5Qti\n2/Zu+dtsIJ4VgQeIXxZzSUNTicM/5wJd88pvRd4Q47S8O/HLchrxy2YscVTExo3sv+B+Mut/SJyf\nZCLwFTEheBg4OK/c6sShrZ8T+9pcRvyArisQ6/HEjo9fACOJnZgfA0YUeA3yh0PPaOC5bATcQ2yC\n+DK9zrXA1nnlfkOcaO1LYkfdQ4kJTYPDoTOP35w4Z85nxC/wF4Ej88psQ5yfZBYxYbkPWC+vzDnp\nOf4gb3nB55mO0csF3gv7EOcB+jjt734yw5ObctxTuZ3T+/jr7GtAbNIZRkwEJwPXEJvN8l+nhfh+\njps52eNK4fPn5+m9OyO9fx4BflXM+5R6zocCz73omBb0dUnLOgAnEfuBfUGcA+k54pw7SzcUq2+V\nf1N6kc2sjVGcifVRYJsQwhPljqetScf3MWCvkBmKbmYtq6L7uEg6TXHq58szyxaTdJWkqYrTUw9M\nM4xmH/cjSYMVpx2fJOni1CvfzMzMqljFfplL+iWxCjl/eGV/Yt+BPYnV86sSOzHmHrcQsdpzYWAz\nYhXngcRptM3am5YYpmpmVjYVmbikHvoDgEOI7di55blr3fQLIYwMIbxIvB7KFpJ+lYptT+ys94cQ\nwqshhGHAWcDRhUZ9mLVxbgtuWT6+Zq2sIhMX4CrgwTD/nBCbEGtSRuQWhDhd+Pt8P6R2M+DVEMLU\nzOOGEWeWnG9YqVlblZL7Du7f0jIyx9f9W8xaUcXVQEjaF/gFMUnJtxLwTZj3+jUQe9ivnP5fmfln\nMJ2cWVffzJ5mZmZW4SoqcUnzE/QHtgshfNuUh1JclW3BMukCXNvz/ZBTMzMzK87ixAugDgshfNrS\nO6uoxIU4sdYKwGilq+kRx+N3l3QMceKqxSQtm1frsiLf16pMAn6Zt92V0t9C15KBmLTcXs86MzMz\na9wfiFcWb1GVlrgMZ/7ZM28mXp30QuLkW98C2xInlELSusRZSp9O5Z8hTr/+w0w/l57EyZXeqGe/\nEwEGDBjA+uuv3xzPo2r169ePK664otxhlJ2Pw/d8LCIfh+/5WEQ+DtGbb77JfvvtB3lXuW8pFZW4\nhBBmk5dcSJoNfBpCeDPd/xdwuaTpxFke/w48FUJ4Pj3k4bSN2ySdQrxmyF+AKxtofvoKYP3116dr\n167N/KyqS8eOHdv9MQAfhywfi8jH4Xs+FpGPw3xapatFRSUu9cjvl9KPOB30QGAx4tTfR39XOIQ6\nSTsTp8N+mjj9+s3EaaTNzMysilV84hJC+G3e/a+BY9Otvsd8QLzeh5mZmbUhlTqPi5mZmdl8nLjY\nPGpqasodQkXwcfiej0Xk4/A9H4vIx6E8fHVoQFJXYPTo0aPd0crMzKwJxowZQ7du3QC6hRDGtPT+\nXONiZmZmVcOJi5mZmVUNJy5mZmZWNZy4mJmZWdVw4mJmZmZVw4mLmZmZVQ0nLmZmZlY1nLiYmZlZ\n1XDiYmZmZlWj4i+yaGZmVmnGjh3LhAkT6Ny5M126dCl3OO2Ka1zMzMyKNG3aNHbYYSd2Wm89Bvbq\nxbrrrssOO+zE9OnTyx1au+HExczMrEi9e/fhxUeeZggrcSKdWYwbGT78v9TU7Ffu0NoNNxWZmZkV\nYezYsTw+bAgj6MKyzGAzHuZr1oK5izBsWB/GjRvnZqNW4BoXMzOzIkwYN46bga68z648wETWSmu2\nAmD8+PG8NcbDAAAgAElEQVTlCq1dcY2LmZlZEX41aBCdgL05nOfYNLNmJACdO3cuS1ztjRMXMzOz\nxtx4I8tfey3Xr/sT7p8wAOb+kljTMpIOHfrSo0cvNxO1EjcVmZmZNWT4cDj8cDjiCPZ65il69NgM\n6AOsAfShR4/NqK0dUOYg2w/XuJiZmdXntddgzz1hu+3gH/+g08ILM3ToYMaNG8f48eM9j0sZOHEx\nMzMr5OOPYaedYK214K67YOHvvzK7dOnihKVM3FRkZmaWb/Zs2GUXmDMHBg2CZZYpd0SWVFTiIukI\nSS9LmpFuT0vaIbP+cUl1mdtcSVfnbeNHkgZLmi1pkqSLJVXU8zQzswo2dy784Q/w1lsweDCsvnq5\nI7KMSmsq+gA4BcgNhj8QuF/SL0IIbwIB+CdwFqBU5ovcg1OCMgT4CNgMWBW4DfgGOLMV4jczs2p3\n0knw4IPx9otflDsay1NRiUsIYXDeojMlHUlMQt5My74IIUypZxPbAz8BtgkhTAVelXQWcKGkc0MI\nc1okcDMzaxuuvBL694erroJevcodjRVQsU0okhaStC+wJPB0ZtUfJE2R9KqkCyQtkVm3GfBqSlpy\nhgEdgZ+2fNRmZla1Bg2Cvn3hhBPgqKPKHY3Vo6JqXAAkbQg8AywOfA78LoTwdlp9O/AesSloI+Bi\nYF1gr7R+ZWBy3iYnZ9a93HKRm5lZ1RozBn7/e9htN7jkknJHYw2ouMQFeAv4ObAcsCdwq6TuIYS3\nQgg3ZMq9LmkSMELSWiGEdxvZbmiheM3MrJp98AHsvDNsuCEMGAALVWxjhFGBiUvqh/JOujtG0q+A\nvsCRBYo/m/52Bt4FJgG/zCuzUvqbXxMzn379+tGxY8d5ltXU1FBTU1Nc8GZmVl1mzoxztSy6KDzw\nACy5ZLkjqmi1tbXU1tbOs2zGjBmtGoNCqOyKCEkjgPdCCAcXWLcF8ATw8xDCa2no9IPAKrl+LpIO\nAy4CVgwhfFvPProCo0ePHk3Xrl1b6qmYmVkl+fbbWNPy7LPw9NOwwQbljqgqjRkzhm7dugF0CyGM\naen9VVSNi6TzgYeIw6KXAf5AvIpVT0lrA72Jw50/JTYnXQ6MDCG8ljbxMPAGcJukU4BVgL8AV9aX\ntJiZWTsUAhxzDDz6KAwd6qSlilRU4kJs1rmVmHDMAF4BeoYQHpW0OtCD2Gy0FDG5uQc4P/fgEEKd\npJ2Ba4gjkWYDNwPntOJzMDOzSnfJJfDPf8JNN8G225Y7GmuCikpcQgiHNLDuf8DWRWzjA2DnZgzL\nzMzaknvugVNOgTPPhAMPLHc01kTuOm1mZu3HM89Anz7Quzecd165o7ESOHExM7P2YcIE2HVX+OUv\n4cYbQWr8MVZxnLiYmVnbN21aHPbcqRP85z+w2GLljshKVFF9XMzMzJrd11/DHnvA1Knw3//C8suX\nOyJbAE5czMys7QoBDj00JiwjRkDnzuWOyBaQExczM2u7/vxnuO02uPNO2GKLckdjzcB9XMzMrG26\n9daYuFxwQbyAorUJTlzMzKztefxxOOQQ+OMf4dRTyx2NNSMnLmZm1ra89Rb87new1VZwzTUe9tzG\nOHExM7O245NPoFcvWG01GDgQFlmk3BFZM3PnXDMzaxu+/BJ22y3+fewx6Nix3BFZC3DiYmZm1a+u\nLk7l/8orMHIkrLlmuSOyFuLExczMqt+pp8K998J998Emm5Q7GmtBTlzMzKy6XXcdXHIJ9O8fm4qs\nTXPnXDMzq15Dh8LRR8Oxx0LfvuWOxlqBExczM6tOL78Me+8NO+4IV1xR7mislThxMTOz6vPRR7Dz\nztClC9TWQocO5Y7IWokTFzMzqy6zZsWkRYJBg2DppcsdkbUid841M7PqMWcO7LsvjB8PTz0Fq65a\n7oislTlxMTOz6hACHH987JA7ZAj87GfljsjKwImLmZlVh/794aqr4J//hJ49yx2NlYn7uJiZWeX7\nz3/gxBPh5JPh0EPLHY2VkRMXMzOrbM8/D717w157wV//Wu5orMycuJiZWeWaOBF22QV+8Qu45RZY\nyF9b7V1FvQMkHSHpZUkz0u1pSTtk1i8m6SpJUyV9LmmgpBXztvEjSYMlzZY0SdLFkirqeZqZWRE+\n+wx22gmWWgruvx+WWKLcEVkFqLQv9A+AU4Bu6fYocL+k9dP6/sBOwJ5Ad2BV4N+5B6cEZQix0/Fm\nwAHAgcB5rRO+mZk1i2++iU1DH38cRxCtsEK5I7IKUVGjikIIg/MWnSnpSGAzSR8CBwP7hhBGAkg6\nCHhT0q9CCM8B2wM/AbYJIUwFXpV0FnChpHNDCHNa79mYmVlJQoAjjoAnnoBHHoH11it3RFZBKq3G\n5TuSFpK0L7Ak8AyxBmZhYESuTAjhbeB9YPO0aDPg1ZS05AwDOgI/bY24zcxsAf31r3DTTXDjjbDV\nVuWOxipMxSUukjaU9DnwNXA18LsQwlvAysA3IYSZeQ+ZnNaR/k4usJ5MGTMzq1S1tXDGGXDuubDf\nfuWOxipQRTUVJW8BPweWI/ZluVVS9wbKCwhFbLfRMv369aNjx47zLKupqaGmpqaIzZuZ2QJ58kk4\n8EDYf384++xyR2MF1NbWUltbO8+yGTNmtGoMCqGY7/zykfQIMB64GxgOdMrWukiaCFwRQvibpD8D\nu4QQumbW/xh4B9g4hPByPfvoCowePXo0Xbt2LVTEzMxa0rhxsNlmsNFGMGwYLLpouSOyIo0ZM4Zu\n3boBdAshjGnp/VVcU1EBCwGLAaOBOcC2uRWS1gXWAJ5Oi54Bfibph5nH9wRmAG+0SrRmZtY0U6dC\nr16w4opw771OWqxBFdVUJOl84CHisOhlgD8AWwE9QwgzJf0LuFzSdOBz4O/AUyGE59MmHiYmKLdJ\nOgVYBfgLcGUI4dvWfTZmZtaor76C3XeHGTPgv/+FTp3KHZFVuIpKXICVgFuJCccM4BVi0vJoWt8P\nmAsMJNbCDAWOzj04hFAnaWfgGmItzGzgZuCcVorfzMyKVVcHBx0Eo0fDY4/B2muXOyKrAhWVuIQQ\nDmlk/dfAselWX5kPgJ2bOTQzM2tuZ58Nd94J99wT+7eYFaEa+riYmVlbc+ONcP75cPHFcYZcsyI5\ncTEzs9Y1fDgcfni8nXRSuaOxKuPExczMWs/rr8Oee0KPHnDllSCVOyKrMk5czMysdUyaFIc9//jH\ncNddsHBFdbO0KuHExczMWt7s2bDLLjBnDgweDMsuW+6IrEo53TUzs5Y1dy784Q/w5pswahSsvnq5\nI7Iq5sTFzMxa1p/+BA8+CA88ABtvXO5orMo5cTEzs5Zz1VVwxRWxI+5OO5U7GmsD3MfFzMxaxqBB\ncNxx0K8fHH104+XNiuDExczMmt+YMbDvvrDbbnDJJeWOxtoQJy5mZta8PvgAdt4ZNtgABgyADh3K\nHZG1IU5czMys+cycGfuyLLpo7Iy75JLljsjaGHfONTOz5jFnDvz+9/D++/DUU7DyyuWOyNogJy5m\nZrbgQoBjjonXIRo6FH7603JHZG2UExczM1twl14K110Xr/q87bbljsbaMCcuZmZWtLFjxzJhwgQ6\nd+5Mly5d4sKBA+Hkk+GMM+Cgg8oboLV5TlzMzKxR06ZNo3fvPgwbNuS7Zdtv34u7T+jLsn36QE0N\n/OUvZYzQ2guPKjIzs0b17t2H4cP/CwwA3gcGMOGRp6jbZRfo1i02EUlljtLaA9e4mJlZg8aOHZtq\nWgYAfwBgOXrxQN1STPlmBp9edhnrLL54WWO09sM1LmZm1qAJEyak/7oDsAjfcC97sAJf0gsYO21a\n2WKz9seJi5mZNWidddZJ/z0BBG7gEH7N0+zO0YwHOnfuXMborL1x4mJmZg1ad9112X77XnTocBzn\nsCf7cxsHcAj/7XA122/f6/vRRWatwImLmZk1qvaO27jxx8tzLvdxGnAXV9Ojx2bU1g4od2jWzlRU\n4iLpNEnPSZopabKk+yStm1fmcUl1mdtcSVfnlfmRpMGSZkuaJOliSRX1XM3MqkYIdLrwQvafMI6p\n/frRfcgQxo4dy9Chg+nUqVO5o7N2ptJGFW0J/AN4gRjbX4GHJa0fQvgylQnAP4GzgNzYuy9yG0gJ\nyhDgI2AzYFXgNuAb4MxWeA5mZm1HXR0ceyxcfTX0788P+/Zlx3LHZO1aRSUuIYRe2fuSDgQ+AboB\nT2ZWfRFCmFLPZrYHfgJsE0KYCrwq6SzgQknnhhDmNH/kZmZt0Jw5cMghcOutcMMN8Mc/ljsis8pq\nKipgOWINS/5Yuz9ImiLpVUkXSFois24z4NWUtOQMAzoCvuqXmVkxvvkGeveGAQPg9tudtFjFqKga\nlyxJAvoDT4YQ3sisuh14j9gUtBFwMbAusFdavzIwOW9zkzPrXm6pmM3M2oQvv4S994ZHHoF//xt2\n263cEZl9p2ITF+BqYANgi+zCEMINmbuvS5oEjJC0Vgjh3Ua2GZo5RjOztmXWrJioPPMMPPgg9OxZ\n7ojM5lGRiYukK4FewJYhhI8bKf5s+tsZeBeYBPwyr8xK6W9+Tcw8+vXrR8eOHedZVlNTQ01NTTFh\nm5lVt88+g1694LXXYNgw2HLLckdkFaa2tpba2tp5ls2YMaNVY1AIlVUJkZKW3YCtQgjvFFF+C+J0\njj8PIbwmaQfgQWCVXD8XSYcBFwErhhC+LbCNrsDo0aNH07Vr12Z8NmZmVWLq1Fi7MnFiTFp+mf/7\nz6ywMWPG0K1bN4BuIYQxLb2/iqpxSfOx1AC7ArMl5WpKZoQQvpK0NtCbONz5U+DnwOXAyBDCa6ns\nw8AbwG2STgFWAf4CXFkoaTEza/c+/hh69IjJy+OPw0YblTsis3pVVOICHEHsh/J43vKDgFuJc7H0\nAPoCSwEfAPcA5+cKhhDqJO0MXAM8DcwGbgbOadnQzcyq0HvvwbbbwldfwRNPwHrrlTsiswZVVOIS\nQmhweHYI4X/A1kVs5wNg52YKy8ysbRo3LiYtiywCo0bBWmuVOyKzRlX6PC5mZtYSXnstdr5daqlY\n0+KkxaqEExczs/Zm9GjYaitYZRUYORJWW63cEZkVzYmLmVl78tRT8NvfQpcu8OijsOKK5Y7IrEmc\nuJiZtRfDh8chz127xllxfWVnq0JOXMzM2oMHH4Sdd45NREOGwDLLlDsis5I4cTEza+vuugv22AN2\n2gnuuw+WWKLxx5hVqCYNh5a0ELAVsCWwJrAkMAV4ERiehiGbmVmluOkmOOSQeKXnm26ChStqFgyz\nJiuqxkXSEpLOJE74NgTYEVgOmEu8RtCfgXclDZG0WUsFa2ZmTXDllXDwwXDooXDLLU5arE0o9l08\nFngGOBR4pJ7r/axJnI7/TknnhxCub74wzcysSS66CE49FU44AS69FKRyR2TWLIpNXHqGEN5sqEAI\n4T3gr5IuBdZY4MjMzKzpQoCzz4b/+7/499xznbRYm1JU4tJY0pJX9ltgQskRmZlZaUKAE0+EK66I\nNS4nn1zuiMyaXZNHFUnaQdJvMvePlvSSpDskeVIAM7NymDsXDj88Ji1XXeWkxdqsUoZDXwIsCyDp\nZ8BlxA67awGXN19oZmZWlDlz4IAD4F//gptvhqOOKndEZi2mlC7mawFvpP/3BAaFEE6X1JWYwJiZ\nWWv5+muoqYkTzN15J+y9d7kjMmtRpSQu3xDnbwHoAdya/p9GqokxM7NW8MUXsOee8NhjcWK5nXcu\nd0RmLa6UxOVJ4HJJTwG/An6flq8L/K+5AjMzswZ8/jnssgs8/zwMHgzbblvuiMxaRSl9XI4B5gB7\nAUeGED5My3cEhjZXYGZmVo/p06FHD3jxRXj4YSct1q40ucYlhPA+MF99ZAihX7NEZGZm9fvkk3iF\n5//9LzYRde1a7ojMWlWxU/4v1ZSNNrW8mZkV4cMPoXt3mDwZRo500mLtUrFNReMlnSpplfoKKNpO\n0kPAcc0TnpmZAfDuu7DllvDllzBqFPz0p+WOyKwsim0q2hq4ADhX0kvAC8DHwFdAJ2ADYHNi35e/\nAtc1e6RmZu3VW2/FPi1LLBGbh9Zcs9wRmZVNsVP+vw3sKWkNYG9gS2ALYAlgKvAi8QKMD4UQ5rZQ\nrGZm7c8rr8SkZcUV4ZFHYJV6K77N2oUmdc5NHXMvSzczM2tJzz0HO+wAa60Fw4bBD39Y7ojMyq6U\n4dBmZtbSnngiDnNef3149FEnLWZJRSUukk6T9JykmZImS7pP0rp5ZRaTdJWkqZI+lzRQ0op5ZX4k\nabCk2ZImSbpYUkU9VzOzeg0bFmtaNt00ztPSsWO5IzKrGJX2Zb4l8A9gU+LlBBYBHpa0RKZMf2An\n4nWSugOrAv/OrUwJyhBiM9hmwAHAgcB5LR++mdkCuu++OCPub38LgwbBUp5dwiyrlCn/W0wIoVf2\nvqQDgU+AbsCTkpYFDgb2DSGMTGUOAt6U9KsQwnPA9sBPgG1CCFOBVyWdBVwo6dwQwpzWe0ZmZk1w\nxx2w//6wxx4wYAAsumi5IzKrOJVW45JvOSAQL+AIMYFZGBiRK5BGPL1PHI4NsZbl1ZS05AwDOgKe\n+MDMKtP118N++0GfPlBb66TFrB4lJS6StpQ0QNIzklZLy/pI+k1zBSZJxGahJ0MIb6TFKwPfhBBm\n5hWfnNblykwusJ5MGTOzytG/Pxx2GBx1FPzrX9ChQ7kjMqtYTW4qkrQncBtwO7AxsFha1RE4HehV\nz0Ob6mrixHbFJEMi1sw0psEy/fr1o2NeJ7iamhpqamqK2LSZWROFABdcAGeeCSefDBdeCFK5ozKr\nV21tLbW1tfMsmzFjRqvGUEoflzOBI0IIt0raN7P8qbRugUm6kpgAbRlC+CizahKwqKRl82pdVuT7\nWpVJwC/zNrlS+ptfEzOPK664gq6+9oeZtYYQ4PTTY7Jy3nkxeXHSYhWu0I/5MWPG0K1bt1aLoZSm\novWAJwosn0Hsk7JAUtKyG7Fz7ft5q0cTLyuwbab8usAawNNp0TPAzyRlJz3omeJ7AzOzcqurg+OO\ni0nLZZfBWWc5aTErUik1LpOAzsDEvOW/Ad5ZkGAkXQ3UALsCsyXlakpmhBC+CiHMlPQv4HJJ04HP\ngb8DT4UQnk9lHyYmKLdJOgVYBfgLcGUI4dsFic/MbIHNnQuHHgo33wzXXRf7tphZ0UpJXK4H/ibp\nYGKfkVUlbQ5cyoLPlXJE2ubjecsPAm5N//cD5gIDif1rhgJH5wqGEOok7QxcQ6yFmQ3cDJyzgLGZ\nmS2Yb7+No4YGDoRbb42jiMysSUpJXC4kNjGNAJYkNht9DVwaQrhyQYIJITTadBVC+Bo4Nt3qK/MB\nsPOCxGJm1qy++gr22QeGDoW7745ztZhZkzU5cQkhBOB8SZcQm4yWBt4IIcxq7uDMzNqE2bNh993h\nySfh/vthxx3LHZFZ1Sp55twQwje4s6uZWcNmzICddoKXX4aHHoKtty53RGZVrZR5XBYnNtNsQxyG\nPE/zTgjB44nNzAA+/RS23x4mTIBHHoHNNit3RGZVr5Qal38RhxcPBJ6juInfzMzal0mTYLvtYPJk\neOwx+MUvyh2RWZtQSuKyM9ArhPBUcwdjZtYmvP8+9OgR+7aMHAnrr1/uiMzajFISlw+J86eYmVm+\n8eNh221hoYVg1ChYe+1yR2TWppQyc+6JwEWS1mzuYMzMqtobb0D37rD44k5azFpIKTUuLwCLA+9I\n+gKYZzbaEMIPmiMwM7OqMmYM9OwJq64aO+KutFLjjzGzJislcakFViNeCXoy7pxrZu3d009Dr16w\n7rpxgrkf+PebWUspJXH5NbB5COHl5g7GzKzqPPoo7LordO0KgwbBssuWOyKzNq2UPi5vAUs0dyBm\nZlVn8OBY07LFFrGmxUmLWYsrJXE5FbhM0taSlpe0bPbW3AGamVWkgQPhd7+DHXaABx6AJZcsd0Rm\n7UIpTUVD098RectF7O/SYYEiMjOrdLfeCgcdBL//PdxyCyyySLkjMms3Sklctmn2KMzMqsU118BR\nR8Ehh8C110IH/1Yza02lXB16ZEsEYmZWqcaOHcuECRPYZORIVrjoIujbF664AqRyh2bW7hSVuEja\nCHgthFCX/q9XCOGVZonMzKzMpk2bRu/efRg2bAjnADsCtWuvww5nn00nJy1mZVFsjctLwMrAJ+n/\nQOzTks99XMyszejduw9PP/IMN7ElBzKK09iHS94bTo/efRg6dHC5wzNrl4pNXNYCpmT+NzNr08aO\nHcsHw4bwX1ZlDcawH7dxO/vB3AEMG9aHcePG0aVLl3KHadbuFJW4hBDey9xdE3g6hDAnW0bSwsTJ\n6bJlzcyqTwh8fc01PA9MYBk2YQRv85O0cisAxo8f78TFrAxKmcflMaDQfNYd0zozs+o1axYccAA/\n69+f24FNOTmTtADE8QmdO3cuS3hm7V0pw6Fz87XkWx6YvWDhmJmV0auvwj77wAcfwIAB/Pu2O/hm\n+J9g7qLEmpaRdOjQlx49erm2xaxMik5cJN2b/g3AzZK+zqzuAGwEPN2MsZmZtY4Q4MYb4ZhjoEsX\nGD0a1luP2l69qKnZj2HD+nxXtEePXtTWDihjsGbtW1NqXGakvwI+B77MrPsG+C9wfTPFZWbWOmbN\ngiOOgNtvh8MOg/79YYl4ObZOnToxdOhgxo0bx/jx4+ncubNrWszKrOjEJYRwEICkicClIYRmbxaS\ntCXwJ6AbsAqwewjhgcz6m4AD8h42NITQK1OmE3AlsDNQB/wb6NsS8ZpZlXvlldg09OGHcMcdUFNT\nsFiXLl2csJhViCZ3zg0h/LkFk4CliPPEHE3hfjQADwErEeeVWRnI/6S5A1gf2BbYCegOXNcSwZpZ\nlQoBrr8eNt0UFl88Ng3Vk7SYWWUppXNuiwkhDCVdxFGqd1rKr0MIUwqtkPQTYHugWwjhxbTsWGCw\npJNCCJNaIGwzqyaffw6HHw61tbGJ6IorYvJiZlWhlOHQ5ba1pMmS3pJ0taTs0OzNgem5pCUZTqy9\n2bRVozSzyvPyy9CtGwwaBHfeGS+Y6KTFrKpUW+LyELA/8FvgZOL4xCGZ2pncZQm+E0KYC0xL68ys\nPQoBrrsuNg0ttVRsGvr978sdlZmVoKKaihoTQrg7c/d1Sa8CE4CtaXjyu/rmnjGztm7mzDha6K67\n4Kij4LLLXMtiVsWKvTr0ccVuMITw99LDaZoQwruSpgKdiYnLJGDFbBlJHYBOwOTGttevXz86duw4\nz7Kamhpq3GnPrDq9+GIcNTR5ckxc9tmn3BGZVbXa2lpqa2vnWTZjxox6SrcMhdB4RYSkd4vcXggh\nrL1gIX23zzryhkMXKLM68dpIu4UQBqXOua8Dm2Q65/YEhgCr19c5V1JXYPTo0aPp2rVrc4RvZuUU\nAlx7LfTrBxtsAHffDZ6i36xFjBkzhm7dukEcGDOmpfdX7EUWW+WK0JKWItae5PqsrC3p58Q+KtOA\nc4jzskxK5S4CxgLDUpxvSRoGXC/pSGBR4B9ArUcUmbUTM2fCoYfGZOXoo+HSS900ZNaGVFofl02I\nTT4h3S5Ly28BjiJeVmB/YDngI2LCcnYI4dvMNnoTJ6AbTpyAbiDQtzWCN7MyGzMmNgdNmQL33AN7\n7VXuiMysmRXbx+XyYjcYQjih1GBCCCNpeKTTDkVs4zNgv1JjMLMqFAJcfTWccAJsuCEMGwbrrFPu\nqMysBRRb47JxkeU8csfMWteMGXDIITBwIBx7LFxyCSy2WLmjMrMWUmwfl21aOhAzsyYbPTo2DX36\nKfz737DHHuWOyMxaWLVNQGdmFpuGrrwSfv1r+MEPYt8WJy1m7UJJnXMl/RLYG1iDOHLnOyEEf3qY\nWcv57DP44x/h3nuhb1+46CI3DZm1I02ucZG0L/AU8QrMvwMWATYgTsPfurPQmFn78sIL0LUrjBgR\nE5f+/Z20mLUzpTQVnQ70CyHsAnxDHGq8PnA38H4zxmZmFoUAf/tbbBr64Q/jjLi/+125ozKzMigl\ncVkHGJz+/wZYKsTpd68ADmuuwMzMAJg+PfZfOf74OKHck0/CWq0yJ6aZVaBS+rhMA5ZJ/38IbAi8\nSpwUbslmisvMDJ57Ll7F+bPP4L77YPfdyx2RmZVZKTUuo4Dt0v/3AH+TdD1QC4xorsDMrB0LIfZf\n+c1vYMUVY9OQkxYzo7Qal2OA3IU/zge+BX5NvIbQ/zVTXGbWXk2fDgcdBPffH2fC/etfYdFFG3+c\nmbULTU5cQgjTMv/XARc2a0Rm1n49+2xsGpo5MyYuu+5a7ojMrMKUMhz6FkkHSVq7JQIys3YoBLj8\n8tg0tMoqsWnISYuZFVBKH5dvgdOA8ZI+kHSbpEMkdW7m2MysPZg2DXbbDU48MY4ceuIJWHPNckdl\nZhWqlKaiQwAkrQpslW4nAtdJ+jiEsHrzhmhmbdYzz8C++8KsWfDgg7DzzuWOyMwq3IJcq+gzYCpx\nePR0YA4wpTmCMrM2rq4OLr0UuneH1VaLTUNOWsysCKX0cTlf0lPAp8DFxLlbLgZWDiFs3MzxmVlb\n8+mnsWnoT3+Ko4ZGjoQ11ih3VGZWJUoZDn0asWblXODeEMK4Zo3IzNqup5+OTUOzZ8OgQbDTTuWO\nyMyqTClNRRsT52/ZFHha0oeSbpd0qKQuzRuembUJdXVwySWxaehHP4KXXnLSYmYlaXLiEkJ4OYTw\n9xDCHiGEFYBexGsWXQ281dwBmlmVmzoVdtkFTj4ZTjoJHn88Ji9mZiUopakISRsDW6fblsCyxOsV\nPd5McZlZW/Dkk1BTA19+CYMHQ69e5Y7IzKpckxMXSdOBpYFXiInKv4AnQgifNW9oZla16urg4ovh\nzDNh882hthZW90wJZrbgSqlx6UNMVGY2dzBm1gZMmQL77w9Dh8Jpp8F558HCJVXumpnNp5QJ6Aa1\nRCBm1gaMGhWbhr7+Gh56CHbYodwRmVkbsyAT0DU7SVtKeiCNVKqTNN/FSiSdJ+kjSV9IeiT/UgOS\nOnv0hkwAACAASURBVKVRTjMkTZd0g6SlWu9ZmLVDdXXxKs7bbANrrx1HDTlpMbMWUFGJC7AU8BJw\nNBDyV0o6BTgGOBz4FTAbGCYpe837O4D1gW2BnYDuwHUtG7ZZOzZlSux0e8YZcOqp8OijcTZcM7MW\nUFENzyGEocBQAEkqUKQv8JcQwoOpzP7AZGB34G5J6wPbA91CCC+mMscCgyWdFEKY1ApPw6z9eOKJ\n2DT07bexT0vPnuWOyMzauEqrcamXpLWAlYERuWWpg/CzwOZp0WbA9FzSkgwn1t5s2kqhmrV9dXVw\n/vmxaahLl9g05KTFzFpB1SQuxKQlEGtYsiandbkyn2RXhhDmEi8EuTJmtuA++ST2XznrLDj9dBg+\nHFZdtdxRmVk7UVFNRSUSBfrDlFDGzPKMHTuWCRMm0LlzZ7p06RJnve3dG+bOhWHDYLvtyh2imbUz\n1ZS4TCImICsxb63LisCLmTIrZh8kqQPQiflraubTr18/OnbsOM+ympoaampqSo/arApNmzaN3r37\nMGzYECBWzd7UuQt93pmAuneHO+6AVVYpb5Bm1upqa2upra2dZ9mMGTNaNQaFUJkVEZLqgN1DCA9k\nln0EXBJCuCLdX5aYkOwfQrhH0k+A14FNMp1zewJDgNXr65wrqSswevTo0XTt2rVFn5dZNfj/9u48\nvIry7OP49wco4AZURFrFDQJoQSu44AaoURSrtbbVolKtWtvqW7e2ai99hVpfrdpad+veKhrF3SqS\nglVcESUtgooh4FIVFAQRFETC8/7xTGQ4JiRAkskkv891nSuZ/Z77zMm588wzMwcddAjjx0+ksvJq\nurA9oziO/ZnG3d17cOyb06F166xDNLMmoqysjP79+0O8MKasobfXpPq4SNpQ0k6SvpOM2i4Zrnoi\n25XA+ZIOldQXuAN4D3gEIIQwHSgFbpa0q6S9gGuAEl9RZFY35eXllJaOobLyag5gM/7DIfRlLgdw\nLsNnVjBj1qysQzSzFqxJFS7ALsTTPpOJfVL+DJQBvwcIIVxGLERuJF5N1B44OISwLLWOo4lPqR4P\nPAY8Q7zvi5nVwcyZM+kDPMZt/JMhvM4OfIf/8C9OAaCioiLbAM2sRWtSfVxCCBOopZgKIYwERq5m\n+ifAsfUamFlL8d//sucttzAFmMU0juRe7uNHxO5lowDo0aPH6tZgZtagmlqLi5llYcECOPtsKCqi\nw7PPckPvHejb6kvuYxnxbOwoWrc+nSFDhsari8zMMuLCxawlW7oU/vQn6N4drrsuFi8VFRz9wnMM\nOmAP4sPgtwKGU1w8gJKSURkHbGYtXZM6VWRmjaSyEu66K95E7v334Wc/gxEjoGu8T2MnYOzYx5kx\nYwYVFRUr7+NiZpYxFy5mLUkI8cZx55wDr74KRxwB//wn9OpV7exFRUUuWMysSfGpIrOWYvJkKC6G\ngw+GTTaBF16ABx6osWgxM2uKXLiYNXezZsUnOO+yC8yeDY88Ep/qvMcetS9rZtbEuHAxa67mzoXT\nT4fevWOhcvPN8fTQYYeBlHV0ZmZrxX1czJqbzz6DK6+ESy+NBcrIkXDGGbDBBllHZma2zly4mDUX\ny5fD7bfHq4PmzYNTToHzz4fOnbOOzMys3vhUkVnehRD7rfTtCyefDIMHw/TpsdXFRYuZNTMuXMzy\n7IUXYJ994PDDYYst4JVX4O67Ybvtso7MzKxBuHAxy6Pp0+M9WPbaCxYvjvdmGTcO4qPlzcyaLRcu\nZnkyezb8/OfQpw+UlcGdd8afBx7oK4XMrEVw51yzPPj0U7j8crjiCmjbFi67LHa+bdcu68jMzBqV\nCxezpmzZMrjxRvjDH2DRonhflnPPhY4ds47MzCwTPlVk1hSFAPfeCzvsEIuV734Xysvhj3900WJm\nLZoLF7Om5qmnYLfd4Mc/jne9ffVVuO026NYt68jMzDLnwsWsqXj1VRg6FPbbD1q1gqefhsceix1x\nzcwMcOFilr1334Xjj4fvfAdmzIDRo2HiRBg0KOvIzMyaHHfONcvKggVwySVw9dWwySZwzTXxzrfr\nrZd1ZGZmTZYLF7PGtnQpXHstXHxxvGro3HPh17+GjTfOOjIzsybPhYtZY6mshLvuig8+/OCD2Lpy\nwQXQtWvWkZmZ5Yb7uJg1tBDgiSegXz847rh4xdBrr8H117toMTNbQy5czBrSK6/A/vvHq4U6dIAX\nX4T774devbKOzMwsl3JXuEgaIWlFwev11PS2kq6TNE/SIkn3S+qSZczWAs2cGe/Dsuuu8OGH8Oij\nMGECDBiQdWRmZrmWu8IlMQ3YHOiavPZOTbsSOAT4ATAQ+BbwQGMHaC3U3Llw2mmw/fbw7LNwyy0w\nZQoceqgfgmhmVg/y2jl3eQhhbuFISZsAJwA/DiFMSMb9FHhD0m4hhEmNHKe1FJ99Bn/5S3z4oQS/\n/328Vf8GG2QdmZlZs5LXFpciSe9LmilplKSqe6H3JxZjT1bNGEJ4E3gX2CODOK25W74cbroJiorg\nwgvhxBPjaaLf/c5Fi5lZA8hj4TIROB4YAvwC2BZ4RtKGxNNGy0IInxYs82Eyzax+hAAPPwx9+8LP\nfw777gtvvhlbXTp3zjo6M7NmK3enikIIpanBaZImAe8ARwJLa1hMQGjo2KyFeOEF+O1v48/i4nhv\nln79so7KzKxFyF3hUiiEsFBSOdADGA+sL2mTglaXLsRWl9U688wz6dChwyrjhg0bxrBhw+ozZMur\n6dPjKaCHH47PFSothQMPzDoqM7NGU1JSQklJySrjFi5c2KgxKIR8N0RI2ojY4nIBcCcwl9g596Fk\nek9gOjCgps65kvoBkydPnkw//+dshWbPhpEj4dZbYcst4aKL4Oij4xOczcxauLKyMvr37w/QP4RQ\n1tDby12Li6TLgX8Qi5UtgN8Dy4F7QgifSroVuELSAmARcDXwvK8osroqLy9n5syZ9Ozale4PPghX\nXAHt2sHll8Mpp0DbtlmHaGbWYuWucAG2BO4GNiW2rjxHbE35OJl+JlAJ3A+0BcYCp2YQp+XM/Pnz\nOfro4bxYOobjgF2AL1q1YsVpp9F+xAjo2DHrEM3MWrzcFS4hhNV2OAkhfAH8KnmZ1c2iRdw0eD9O\nmfo6j9CGNqzgb+zDhUxh+zfKGeuixcysSfBJemu5Fi+Ge++FI45gRefOnDt1CpuxNedwOVvzDifx\nNO+uuIbS0jHMmDEj62jNzAwXLtbSfP55fMjhj34EXbrE5wm99x7lw4ezNbAn/+IqzuB9tkwWGARA\nRUVFZiGbmdlKLlys+VuyBB56KBYpm20Wi5ZZs2DEiPhz0iRanX027wLwTMHCEwDo0aNHIwdtZmbV\nyV0fF7M6+eKLeJ+Ve++NT2ZevBh22gnOOw+OPBIKCpGePXsyZMhQxo8/jcrKQGxpmUDr1qdTXDyU\noqKiTHbDzMxW5cLFmo9ly2DcuFisPPIIfPop9OkDZ58di5VevVa7eEnJKIYNO5bS0uFfjSsuHkpJ\nyaiGjtzMzOrIhYvl25dfwpNPxmLl4Yfhk0+gd28488xYrOywQ51X1alTJ8aOfZwZM2ZQUVFBjx49\n3NJiZtbEuHCx/Fm+HJ56CkaPhgcfhPnz49OZ/+d/YrHSpw9Ia736oqIiFyxmZk2UCxfLh8pKmDAh\nFisPPADz5sF228UnMx95ZOy/sg7FipmZ5YMLF2u6KivhuedisXL//fDRR7DNNnDCCbFY6dfPxYqZ\nWQvjwsWalhUr4IUXVhYrs2dDt24wfHgsVnbd1cWKmVkL5sLFshcCTJwYi5X77oP334cttoCjjorF\nyu67+0nMZmYGuHCxrIQAL7+8slh5913o2jXeHO7II2HPPV2smJnZ17hwscYTApSVxWJl9Gh4++14\nJ9sf/jC2ruy9N7RunXWUZmbWhLlwsYYVAkyZsrJYmTkTNt0UfvCDWKwMHAhtfBiamVnd+BvD6l8I\nMG3aymKlvBw6dYIjjoAbboDBg2G99bKO0szMcsiFi9WfN96Id7AdPTr+3qEDfP/7cOWVUFzsYsXM\nzNaZCxdbN+XlK4uVadNg443h8MPhssvggAOgbdusIzQzs2bEhYutuYqKlaeBpkyBjTaCww6Diy6C\nIUOgXbusIzQzs2bKhYt9pby8nJkzZ1b/cMG33lpZrJSVwQYbwKGHwgUXwMEHQ/v22QRtZmYtigsX\nY/78+Rx99HBKS8d8NW7IkKHcc/kf6VhaGouVl1+Oxckhh8C558LQobDhhhlGbWZmLZELF+Poo4cz\nfvxEYBRbUMSPuJqjSu+hY+mY2Edl6FA46yz47nfjaSEzM7OMuHBpqVasgLfe4v3SUvqXjuE4BtCX\nS+jDa3zB+oxlJ46hjAtffJHuO++cdbRmZmaAC5fmLwSYMyde8TN1avw5bRq89hp8/jlbAL8GprGC\nCQziMs7mUQ5jIYuArTh2zhy6Z7wLZmZmVZpt4SLpVOA3QFdgCvCrEMLL2UbVwD75JBYk6QJl2jT4\n+OM4vX17+Pa3oU+feNfavn2Z2b49PQYOBE4Djkmt7DEAevTo0dh7YWZmVqNmWbhIOgr4M3AyMAk4\nEyiV1DOEMC+ruFZ71c6aWLIk3uCtqjCpKlTeey9Ob90aevWKBUpxcfzZpw9su+3XngXUndgRd/z4\n06isDMAgYAKtW59OcfHQdYvTzMysnjXLwoVYqNwYQrgDQNIvgEOAE4DLGjuYmq7aKSkZRadOnWpe\ncPnyeM+UwtM8FRWxjwrEYqRPHxg+PP7s2xd69lyjG7+VlIxi2LBjKS0d/tW44uIYn5mZWVPS7AoX\nSesB/YGLq8aFEIKk8cAeWcSUvmoHBgLPMH78aQwbdixjxz4e+6H897+rFidTp8ZWlWXL4ko23zwW\nJgcfHIuTPn1ghx3inWrXUadOnRg79nFmzJhBRUXFurcImZmZNZBmV7gAnYHWwIcF4z8EejV2MOXl\n5UlLyyjgGDozlz58iz6Vh9C39E6W9OtH+4oKWLQoLrDxxrEo2X13OPHElad5NtuswWMtKipywWJm\nZk1acyxcaiIgNPZGZ86cmfw2kJGMYAQXArCUtrwBzO/YkS3OO29lK0q3biA1dphmZma50BwLl3lA\nJbB5wfgufL0VZhVnnnkmHTp0WGXcsGHDGDZs2FoH07171cXEz/AQ32cqfZlGHyp4iUqOp/zGG8Gt\nHGZmlgMlJSWUlJSsMm7hwoWNGoNCaPRGiAYnaSLwUgjh9GRYwLvA1SGEy6uZvx8wefLkyfTr16/e\n4znooEMYP34ilZVXsepVOwNiHxczM7OcKisro3///gD9QwhlDb29Vg29gYxcAZws6SeSegN/BTYA\n/pZFMCUloyguHgAMB7YChlNcPMBX7ZiZma2h5niqiBDCaEmdgQuJp4z+AwwJIczNIh5ftWNmZlY/\nmmXhAhBCuB64Pus40nzVjpmZ2bpprqeKzMzMrBly4WJmZma54cLFzMzMcsOFi5mZmeWGCxczMzPL\nDRcuZmZmlhsuXMzMzCw3XLiYmZlZbrhwMTMzs9xw4WJmZma54cLFzMzMcsOFi5mZmeWGCxczMzPL\nDRcuZmZmlhsuXMzMzCw3XLiYmZlZbrhwMTMzs9xw4WJmZma54cLFzMzMcsOFi5mZmeWGCxczMzPL\nDRcuZmZmlhsuXMzMzCw3XLiYmZlZbuSqcJH0tqQVqVelpLML5tlR0jOSlkh6R9Jvs4o3j0pKSrIO\noUlwHlZyLiLnYSXnInIespGrwgUIwPnA5kBX4JvANVUTJW0MlAJvAf2A3wIjJZ3U+KHmkz+IkfOw\nknMROQ8rOReR85CNNlkHsBYWhxDm1jDtWGA94MQQwnLgDUk7A2cBtzRWgGZmZtYw8tbiAnCupHmS\nyiT9RlLr1LQBwDNJ0VKlFOglqUPjhmlmZmb1LW8tLlcBZcB8YE/gj8RTRr9JpncFZhUs82Fq2sJG\niNHMzMwaSOaFi6RLgHNWM0sAtg8hlIcQrkyNnybpS+Cvkn4XQviypk2k1lOTdgBvvPFGXcNuthYu\nXEhZWVnWYWTOeVjJuYich5Wci8h5iFLfne0aY3sKYXXf540QgLQpsGkts80qOP1TtewOwFSgdwhh\nhqS/AxuHEI5IzTMYeBL4Rgih2hYXSUcDd63lLpiZmRkcE0K4u6E3knmLSwjhY+DjtVx8Z2AF8FEy\n/CJwkaTWIYTKZNyBwJs1FS2JUuAY4G1g6VrGYmZm1hK1A7Yhfpc2uMxbXOpK0gBgd+ApYBGxj8sV\nwOMhhBOSeTYBpgPjgEuBvsCtwOkhhFuziNvMzMzqT54Kl52B64FeQFvivVruAP6S7t8iqS9wLbAr\nMA+4OoTwp8aP2MzMzOpbbgoXMzMzszzex8XMzMxaqBZfuEg6VdJbybONJkraNeuY6pOkEQXPd1oh\n6fXU9LaSrktu6rdI0v2SuhSso5ukxyV9JmmOpMskNeljR9I+kh6V9H6yz4dVM8+Fkj6Q9LmkcZJ6\nFEzvJOkuSQslLZB0i6QNC+Zp8s/Gqi0Xkm6v5hgZUzBP7nMh6XeSJkn6VNKHkh6S1LNgnnr5PEga\nLGmypKWSyiUd1xj7WBd1zMPT+vpz4a4vmCfXeQCQ9AtJU5LjeqGkFyQdlJre7I8HqFMemtbxEEJo\nsS/gKOJVRD8BegM3Em9u1znr2OpxH0cArwKbAV2S1zdS028gXk01iHiV1gvAs6nprYiXnJcSOzsP\nIV7FdVHW+1bLfh8EXAgcDlQChxVMPyd5rw8F+gAPAzOB9VPzPEG84eEuxM7g5cCo1PSNgdnA34Ht\ngSOBz4CTst7/NczF7cDjBcdIh4J5cp8LYAwwPImvL/BYcuy3r8/PA/HqisXAZcQ+eacCXwIHZJ2D\nNcjDU8BfC46JjZpTHpIYD0k+Hz2S10XAF8R7h7WI46GOeWhSx0PmCcv4zZoIXJUaFvAecHbWsdXj\nPo4AymqYtklycH4/Na4X8RLz3ZLhg5ODq3Nqnp8DC4A2We9fHXOwgq9/WX8AnFmQiyXAkcnw9sly\nO6fmGQIsB7omw78kdgBvk5rnEuD1rPd5DXNxO/Dgapbp3Uxz0TnZr71Tx8A6fx6IVzS+WrCtEmBM\n1vtclzwk454CrljNMs0uD6kYPwZ+2lKPh8I8NMXjoUk39zckSesB/Yk3pwMgxEyOB/bIKq4GUpSc\nJpgpaZSkbsn4/sR7+aRz8CbwLitzMACYGkKYl1pfKdAB+HbDh17/JG1LfAREer8/BV5i1f1eEEL4\nd2rR8cQ7MO+emqe5PBtrcHLaYLqk6yV9IzVtD5pnLjoS92F+Mlxfn4cBxPxQME9T/btSmIcqx0ia\nK2mqpIsltU9Na3Z5kNRK0o+BDYj3BGuRx0NBHl5ITWoyx0OLLVyI/2W0ZuWzjKp8SPxSay4mAscT\n/0P+BbAt8EzSP6ErsCz50k5L56Ar1ecI8punrsQ/1Kt777uy8saGAIR4U8P5NL/cPEE8XbofcDax\nWXyMpKrHZTS7XCT7diXwXAihqs9XfX0eappnE0lt1zX2+lRDHiDeSfxYYDBwMfHU0p2p6c0mD5L6\nSFpEbF25ntjCMp0WdjzUkIc3k8lN6njI/M65TZBY/XONciWEkL6T4TRJk4B3iH0QarpLcF1z0Gzy\nlKjLftc2T12ejdWkhBBGpwZfkzSV2N9nMLGJuCZ5zsX1wA7A3nWYtz4+D001F1V52Cs9MoRwS2rw\nNUlzgCclbRtCeKuWdeYtD9OBnYgtTz8A7pA0cDXzN9fjodo8hBCmN7XjoSW3uMwjdlTcvGB8F75e\nFTYbIT76oJzYAWsOsL7iHYfT0jmYw9dzVDWc1zzNIX5gVvfez0mGvyKpNdApmVY1T3XrgPzmhuQP\n0TziMQLNLBeSrgWGAoNDCB+kJq3r56G2XHwaQli2LrHXp4I8zK5l9peSn+ljolnkIYSwPIQwK4RQ\nFkI4D5gCnE4LOx5Wk4fqZHo8tNjCJcS77U4G9q8alzSb7s+q5/WaFUkbAd2JnVMnEztYpnPQE9iK\nlTl4EegrqXNqNQcCC4F003JuJF/Mc1h1vzch9tdI73dHxTs2V9mfWPBMSs0zMPkSr1KXZ2M1aZK2\nJD74tOrLrNnkIvmy/h6wbwjh3YLJ6/p5eCM1z/6s6sBkfJNQSx6qszPxv+L0MZH7PNSgFfHu7C3m\neKhBVR6qk+3xkHXP5SxfxNMlS1j1cuiPgc2yjq0e9/FyYCCwNfEy1nHE/xY2TaZfT3x8wmBiZ7Tn\n+frlflOI/SB2JPaV+RD4Q9b7Vst+b0hs9vwO8SqAM5Lhbsn0s5P3+lDi5XsPAzNY9XLoMcArxMdH\n7AW8CdyZmr4JsQD8O7G5/Sji5X4nZr3/dc1FMu0yYtG2NfEPyyvEPzbrNadcJMf6AmAf4n9+Va92\nBfOs0+eBlZd9Xkq8CuUUYBlQnHUO6pIHYDvgfKBfckwcBlQA/2pOeUhi/D/i6cKtibdFuIRYrOzX\nUo6H2vLQFI+HzBOW9StJ3tvEAuZFYJesY6rn/SshXuK9hNgb/m5g29T0tsA1xFMDi4D7gC4F6+hG\nvNfD4uRgvBRolfW+1bLfg4hf0pUFr9tS84wkftl+Tuzd3qNgHR2BUcT/GhYANwMbFMzTF5iQrONd\n4DdZ7/ua5IL4VNexxBaopcAs4r0rNitYR+5zUUMOKoGfpOapl89DkvPJyeduBjA86/2vax6ALYGn\ngbnJe/km8Ytso4L15DoPSXy3JMf8kuQz8E+SoqWlHA+15aEpHg9+VpGZmZnlRovt42JmZmb548LF\nzMzMcsOFi5mZmeWGCxczMzPLDRcuZmZmlhsuXMzMzCw3XLiYmZlZbrhwMTMzs9xw4WJmZma54cLF\nrIFIekrSFVnHkSbpJkkfS6qUtGPW8VjjkfSWpNOyjsNsXblwMWshJB1EfKDoUOCbwLSM4lgh6bAs\ntl2fJN0u6cGs48iSc2BZaJN1AGZWd5JaASGs3UPGegCzQwgv1XNYZmaNxi0u1qwlp2uuknRpcopk\ntqQRqelbJy0AO6bGdUjGDUyGByXDB0oqk/S5pPGSNpN0sKTXJS2UdJekdgUhtJF0jaRPJM2VdGFB\nfOtL+pOk9yQtlvSipEGp6cdJWiDpUEmvEZ/g3K2GfR0k6SVJSyV9IOmSpNBB0u3A1cBWyb7MqmEd\nW0l6VNL8JJ6pSUtN1fQ+ksZIWiRpjqQ7JG26Bvl+CwjAw4VxSPqepMmSlkiqkHSBpNap6SsknSjp\nQUmfSSqXdGhB/DtI+kfyfnwqaYKkbVPTT0reryXJz1+mpq0n6dokd0skzZJ0Tg15GgEcB3wviasy\ndbz0lfRkcpzMk3SjpA2rW09d8irpZEnvVbPMo5JuTn7fTtLDybKLJE2StP9qtleX476VpFuSPHwu\nabpSp5pqycGWku5Njt15SWxbry4HZnWW9eO0/fKrIV/AU8AC4H+B7sBwoBLYP5m+dTK8Y2qZDsAK\nYGAyPCgZfh4YAOwElCfrfgLYEdiL+Nj33xZs+1PgCqAIGEZ85PuJqXluBp4F9gS2Bc4iPjq+ezL9\nOOCLZJ4ByXraVbOf30rWfTXQEzgM+Ai4IJm+MXA+8A6wGbBpDfl6DBgL7ABsQzyttHcqLx8Cf0ji\n2CmZ98k1yHfnJJfDgS5VcQB7A58Axybvyf7ATOB/U+tekcR/JLAdcGWS346pHMwDRgM7E1uYjgOK\nkunHAO8B30u2cXjyng1Ppv8GeDt5L7olP4+qIU8bAvcAjyf57EJswW6fbGM0sD0wONmP21ZzjK42\nr0AnYAmwb2qZjsQidlAyvCPws+R96w78HvgM2DK1zFvAaWtw3LcBRgD9kvmHAYuAH9aSgzbAa8BN\nSTy9gDuBN4A2Wf9N8Cv/r8wD8MuvhnwlX6QTCsa9BFyc/L518se6tsKlEhicmuecZNzWqXE3AGMK\ntj2tYNuXVI0DtgK+BLoWzDMOuCj5/bhkO31q2c//A14vGPdLYGFq+HRgVi3rmUKqWCiYdh7wRMG4\nLZNc9ahLvpPhFcBh1ezzOQXjjgHeL1huZGp4gyQ3BybDFwMVQOsa4p9BQSGS7NNzye9XAePW4Ni6\nHXiwYNzPiMVTu9S4g5P3ebN1yOvDwM2p6ScD/60lvqnAKanhwsJltcd9Deu8BhhdSw6OqeZYXJ9Y\nSBXXNb9++VXTy31crCV4tWB4NvG/wzU1NfX7h8DnIYR3CsbtWrDMxILhF4GzJAnoA7QGypPhKusT\nv/yqLAsh1NaRtney7rTngY0kbRlC+NqphhpcDdwgaQgwHngghFC13zsB+0laVLBMIP6XX5EMr02+\ndwL2lHR+alxrYH1J7UIIS5NxX70HIYTPk1i6pNbxbAihsnDlkjZIYrxV0i0F2/gk+f1vwDhJbxJb\nPB4LIYyrJe5CvYEpqXghvg+tiS0Pc6tZpi55vQu4UdIpIYQvgaOBktT+bUhsZanqeN0GaEcsjtea\npFOBnybraU88Nv9dy2I7AUXV7E/bZH/Gr0tMZi5crCX4smA4sLJ/14rkZ7pwWK8O6wm1rLcuNgKW\nE5viVxRMW5z6fUkd1qVk+4XjqGZ8jUIIt0oaCxwCHAj8TtJZIYTrkngfBc5m1XxBLE6qrE1eNgIu\nAL52hUpBEbC6da8uTxslP08CJhVMq0y2829J2xBbSIqB0ZLGhRCOrCX2tOreh3SsNcVWW17/AdwC\nHCLpFWAfYgtalT8TT6/9mnhqagnwALHQqE6tx72kHwOXA2cSC/BFSYy71bDO9P68QiyuCvenusLN\nbI24cLGWruoP6TeJp0kg9o9Ym6t2qjOgYHgPYEYIIUj6N/E/8c1DCM+v43ZeB44oGLcXsCiE8P6a\nrCiZ/ybgJkkXE09/XAeUJdt4J4RQWGitiS+J+51WBvQKIVTbabiOXgV+Iql1YatLCOEjSe8T+w7d\nU9MKQgiLgfuA+yQ9ADwhqWMI4ZNqZl9WzX68nsTQPoRQVUjtTSyOymvYbK15DSEsVbzs+FhiCyLs\nuQAAArpJREFUP5jpIYQpqVn2BP4WQngUQNJGxD5KNanLcb8n8HwI4caqEZK6F6ynuhyUEfshzU3y\naVavfFWRtWjJf/MTgXMk9Va8oucP1cxa+J9jXXVTvGqop6RhwP8QO5USQpgB3A3cIen7kraRtJuk\ncyUdvIbbuT7Z1jWSekn6HjCS+J94nUn6i+LVU9tI6gfsS/wyhli8fAO4R9IuyZUsQyTdVnCqqzZv\nA/tL2lxSx2TchcQv/AuSK4N6SzpKUnXvRU2uBTYB7pXUX1IPScdKKkqmjyS2IP1KUlFyJc/xks5I\n9v2MZJu9JPUkfvnOqaFoqdqPHZP3dlNJbYindJYCf5f0bUn7Ek+/3RFCqKm1oa55vYvYEnYCMKpg\nHTOAIyTtJGmnZN4a35M6HvczgF2S46FI8Yq4wlOhNeVgHvCIpL2TY2mw4tVm36opJrO6cuFizV1d\nWk5OIDapv0K8Aui8tVxPdcvcQewbMInYsfEvIYR0H4vjk3n+BEwHHgJ2Ad5dow2F8AGxf8OuwH+I\nhczNxE67a6I1sQB4HRiTxHRqso3ZxFacVkApsYXjCmBBCKEqP3XJ06+BA4j7WJas+5/Ad5Pxk4j9\ndc4gfjF+tZvVrOurcSGE+cB+xKtdnia+nyeRnF4KIdyaDP80if1pYufnt5JVLCZ2un6Z2KF4K2JO\na3Iz8GaynY+APZNWliHEQmQS8eqiccCvalpJHfMK8C9gPrHF5e6C1ZxFvJrreeARYh+dssJNFQzX\ndtzfSDx1dw+xyPkGschKqykHA4nv7wPEY+lmYh+XT6vPglndadXPhZmZmVnT5RYXMzMzyw0XLmZm\nZpYbLlzMzMwsN1y4mJmZWW64cDEzM7PccOFiZmZmueHCxczMzHLDhYuZmZnlhgsXMzMzyw0XLmZm\nZpYbLlzMzMwsN1y4mJmZWW78P2Qjr+9XWKu8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba718748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time required for the similarity calculations is estimated at 5.0 hours.\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "nr_of_sentences = [200, 500, 1000, 1500, 2000, 2500, 3000]\n",
    "wall_time = [1.73, 10.2, 41.1, 96, 166, 264, 369]\n",
    "\n",
    "\n",
    "# regression\n",
    "coef = np.polyfit(nr_of_sentences, wall_time, 2)\n",
    "model = np.poly1d(coef)\n",
    "print('equation = {}x^2 + {}x + {}'.format(round(coef[0],5),round(coef[1],2),round(coef[2],2)))\n",
    "wall_time_pred = model(nr_of_sentences)\n",
    "\n",
    "y_res = wall_time - wall_time_pred\n",
    "SSresid = np.sum(np.square(y_res))\n",
    "SStotal = len(wall_time) * np.var(wall_time)\n",
    "\n",
    "# calculate R^2\n",
    "Rsq = 1 - SSresid/SStotal\n",
    "print('R^2 = {}'.format(round(Rsq,4)))\n",
    "\n",
    "# visualise in a plot\n",
    "plt.scatter(nr_of_sentences, wall_time)\n",
    "plt.plot(nr_of_sentences, wall_time_pred, \"r\")\n",
    "plt.title('Relationship between the number of sentences \\n and the required computation time')\n",
    "plt.ylabel('wall time (s)')\n",
    "plt.xlabel('number of sentences to evaluate')\n",
    "plt.show()\n",
    "\n",
    "# predict how long it would take\n",
    "print('The time required for the similarity calculations is estimated at {} hours.'.format(round(model(len(all_sentences))/3600),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue points are the original data points, whereas the red line is the fit.\n",
    "\n",
    "We calculate the similarities between every pair (= amount of sentences^2 times), but the similarity of sentence_X & sentence_Y is equal to the calculation between the reverse, sentence_Y & sentence_X. Half of the calculations would thus be unnecessary. Therefore the computational complexity is is equal to about ((amount of sentences)^2)/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the computed similarity matrix, we will cluster the sentences. We will perform an iterative clustering approach with as stopping criterium a threshold of minimum within-cluster similarity. For good results (see further) we will cluster sentences that are very similar. Therefore, to save memory space, we will not save a pairwise similarity score if the similarity is low (<0.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_similarity_matrix(all_sentences):\n",
    "    core_similarities = list()\n",
    "    for index1, (id1, sentence1, core_sentence1) in enumerate(all_sentences):\n",
    "        for (id2, sentence2, core_sentence2) in all_sentences[index1+1:]: \n",
    "            # using index1+1 because this makes sure that we dont make the same calculation twice \n",
    "            # and that we don't compare with self (would be similarity of 1 anyway)\n",
    "            d = pairwise_similarity(core_sentence1, core_sentence2)\n",
    "            if math.isnan(d) or d<=0:\n",
    "                d = 1e-5\n",
    "            # to save memory space we won't save similarities smaller than 0.5 since this is way below our clustering\n",
    "            # stopping criterium\n",
    "            if d > 0.5:\n",
    "                core_similarities.append((d, id1, id2))\n",
    "    print('Complete!\\n')\n",
    "    print(len(core_similarities), 'similarity scores saved.')\n",
    "    return core_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to start the computation of the pairwise similarity scores between the simplified sentences. We will compute the similarity (1-distance) between every pair of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity_matrix = compute_sentence_similarity_matrix(all_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"docs/similarity_matrix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the magic time function, the computation took 5h 12min to complete on my machine, which is not too far from the estimate (5 hours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.5 Clustering step 5: The actual clustering\n",
    "\n",
    "Merging the sentences together in clusters is best done using 'disjoint sets'. This is an important concept in computer science, especially when designing algorithms. More information can be found on the wikipedia page of disjoint sets:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Disjoint-set_data_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily work with and implement the typical characteristics of disjoint sets, we define the disjoint_forest class below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class disjoint_forest():\n",
    "    \"\"\"disjoint_forest class to easily work with disjoint set\"\"\"\n",
    "    def __init__(self, sentences):\n",
    "        self._ids = sentences\n",
    "        self._parent = {sentence : sentence for sentence in sentences}\n",
    "        self._rank = {sentence : 0 for sentence in sentences}\n",
    "    \n",
    "    def find(self, x):\n",
    "        if self._parent[x] is not x:\n",
    "            self._parent[x] = self.find(self._parent[x])\n",
    "        return self._parent[x]\n",
    "            \n",
    "    def union(self, x, y):\n",
    "        xpos = self.find(x)\n",
    "        ypos = self.find(y)\n",
    "        if xpos == ypos:\n",
    "            return\n",
    "        if self._rank[xpos] < self._rank[ypos]:\n",
    "            self._parent[xpos] = ypos\n",
    "        elif self._rank[xpos] > self._rank[ypos]:\n",
    "            self._parent[ypos] = xpos\n",
    "        else:\n",
    "            self._parent[ypos] = xpos\n",
    "            self._rank[xpos] += 1\n",
    "            \n",
    "    def return_clusters(self):\n",
    "        self._clusters = dict()\n",
    "        for i in self._ids:\n",
    "            if self.find(i) in self._clusters:\n",
    "                self._clusters[self.find(i)].update([i])\n",
    "            else:\n",
    "                self._clusters[self.find(i)] = set([i])\n",
    "                \n",
    "        all_clusters = list()\n",
    "        for cluster in self._clusters:\n",
    "            all_clusters.append(self._clusters[cluster])\n",
    "        return all_clusters\n",
    "        \n",
    "# ref course mathematical optimization (Ghent University)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cluster similar sentences by grouping the most similar sentences first. Then looking at the second most similar sentences and grouping these and continuing this process until the similarity between the next most similar sentences is below the threshold of minimum similarity. This is considered a form of hierarchical clustering. \n",
    "\n",
    "However, I don't want to do the similarity score calculation (5 hour) + clustering every time I work in this notebook. Therefore, I will save the output variable using the standard **pickle** library in python:\n",
    "\n",
    "- pickle.dump(object, filename) to store the variable\n",
    "- pickle.load(filename) to load the variable back in\n",
    "\n",
    "pickle.dump is added to save the cluster dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(all_sentences, core_similarities, min_similarity = 1e-5):\n",
    "    \"\"\"\n",
    "    Clustering using the similarities between the sentences. \n",
    "    \n",
    "    Args:\n",
    "    min_similarity: the minimum within-cluster similarity\n",
    "    \n",
    "    Returns:\n",
    "    cluster dictionary\n",
    "    \"\"\"\n",
    "    # get all sentences ids\n",
    "    all_ids = [sentence_id for (sentence_id,_,_) in all_sentences]\n",
    "\n",
    "    # put all these in seperate trees\n",
    "    forest = disjoint_forest(all_ids)\n",
    "\n",
    "    # get all similarity scores\n",
    "    similarities = sorted(core_similarities.copy(), key = lambda x: x[0], reverse=True)\n",
    "    \n",
    "    amount_of_clusters = len(all_sentences)\n",
    "    while similarities[0][0] > min_similarity:\n",
    "        # to cluster\n",
    "        _, s1, s2 = similarities.pop(0)\n",
    "        forest.union(s1, s2)\n",
    "        \n",
    "    return forest.return_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now applying the cluster function above and saving the clusters dictionary afterwards (using pickle). We will test different minimum within-cluster similarity thresholds and save these to see which performs best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for min_similarity in (0.97, 0.95, 0.92, 0.9):\n",
    "    # computing\n",
    "    print('{} Starting computation for similarity threshold of {}'.format(time.strftime('%H:%M'), min_similarity))\n",
    "    clusters = cluster(all_sentences, similarity_matrix, min_similarity = min_similarity)\n",
    "    # saving\n",
    "    with open('variables/clusters_{}.pickle'.format(min_similarity), 'wb') as outfile:\n",
    "        pickle.dump(clusters, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pick what threshold is best, we will return the distributions of the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_distribution(cluster_filename, plot=False, ylim=0):\n",
    "    '''Plot the size distribution of the clusters.'''\n",
    "    with open(cluster_filename, 'rb') as infile:\n",
    "        cluster = pickle.load(infile)\n",
    "    sorted_clusters = sorted(cluster, key=lambda x: len(x), reverse=True)\n",
    "    sizes = dict()\n",
    "    for c in cluster:\n",
    "        size = len(c)\n",
    "        if size in sizes:\n",
    "            sizes[size] += 1\n",
    "        else:\n",
    "            sizes[size] = 1\n",
    "    if plot:\n",
    "        plt.bar(list(sizes.keys()), sizes.values(), color='g')\n",
    "        plt.xlabel('size of cluster')\n",
    "        plt.ylabel('amount of clusters with this size')\n",
    "        if ylim != 0:\n",
    "            plt.ylim(0, ylim)\n",
    "        plt.show()\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 12208,\n",
       " 2: 843,\n",
       " 3: 278,\n",
       " 4: 104,\n",
       " 5: 61,\n",
       " 6: 43,\n",
       " 7: 28,\n",
       " 8: 20,\n",
       " 9: 20,\n",
       " 10: 15,\n",
       " 11: 6,\n",
       " 12: 6,\n",
       " 13: 2,\n",
       " 14: 3,\n",
       " 15: 5,\n",
       " 16: 5,\n",
       " 17: 1,\n",
       " 18: 4,\n",
       " 19: 3,\n",
       " 21: 3,\n",
       " 22: 3,\n",
       " 23: 1,\n",
       " 24: 2,\n",
       " 26: 2,\n",
       " 27: 1,\n",
       " 29: 1,\n",
       " 31: 1,\n",
       " 32: 4,\n",
       " 35: 2,\n",
       " 38: 1,\n",
       " 44: 1,\n",
       " 45: 1,\n",
       " 84: 1,\n",
       " 94: 1,\n",
       " 2992: 1}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_distribution('variables/clusters_0.9.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the number on the left  = size of the cluster\n",
    "\n",
    "the number of the right = amount of clusters of this size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this distribution tells us is that for a **similarity threshold of 0.9**, there are **12208 clusters of size 1** (non-clustered sentences) and **1 cluster with 2992 sentences**. It's really unlikely that there are 2992 sentences in one cluster, so we continue to the next threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 13802,\n",
       " 2: 881,\n",
       " 3: 278,\n",
       " 4: 114,\n",
       " 5: 76,\n",
       " 6: 40,\n",
       " 7: 31,\n",
       " 8: 22,\n",
       " 9: 20,\n",
       " 10: 11,\n",
       " 11: 6,\n",
       " 12: 8,\n",
       " 13: 7,\n",
       " 14: 8,\n",
       " 15: 5,\n",
       " 16: 2,\n",
       " 17: 3,\n",
       " 21: 1,\n",
       " 22: 1,\n",
       " 23: 1,\n",
       " 24: 2,\n",
       " 25: 1,\n",
       " 26: 3,\n",
       " 27: 1,\n",
       " 28: 1,\n",
       " 34: 2,\n",
       " 45: 1,\n",
       " 46: 1,\n",
       " 50: 1,\n",
       " 71: 1,\n",
       " 105: 1,\n",
       " 176: 1,\n",
       " 1221: 1}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_distribution('variables/clusters_0.92.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is **still one big cluster**, with **1221 sentences in one cluster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAF5CAYAAAAyBjhMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1AiTDEmS1AiTDEmS1AiTDEmS1Ij/D1IdPEzVP4HhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba87a198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{1: 15634,\n",
       " 2: 868,\n",
       " 3: 251,\n",
       " 4: 105,\n",
       " 5: 54,\n",
       " 6: 40,\n",
       " 7: 31,\n",
       " 8: 14,\n",
       " 9: 16,\n",
       " 10: 12,\n",
       " 11: 9,\n",
       " 12: 3,\n",
       " 13: 5,\n",
       " 14: 3,\n",
       " 15: 4,\n",
       " 16: 4,\n",
       " 17: 2,\n",
       " 18: 2,\n",
       " 19: 1,\n",
       " 21: 2,\n",
       " 22: 1,\n",
       " 23: 2,\n",
       " 24: 1,\n",
       " 25: 2,\n",
       " 26: 2,\n",
       " 28: 1,\n",
       " 32: 1,\n",
       " 33: 2,\n",
       " 41: 1,\n",
       " 45: 1,\n",
       " 49: 2,\n",
       " 87: 1}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_distribution('variables/clusters_0.95.pickle', plot=True, ylim=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks more like it. The largest cluster is only 87 sentences big! **0.95 seems to be the ideal threshold so far.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 16346,\n",
       " 2: 771,\n",
       " 3: 222,\n",
       " 4: 87,\n",
       " 5: 58,\n",
       " 6: 38,\n",
       " 7: 25,\n",
       " 8: 12,\n",
       " 9: 12,\n",
       " 10: 9,\n",
       " 11: 9,\n",
       " 12: 2,\n",
       " 13: 6,\n",
       " 14: 1,\n",
       " 15: 3,\n",
       " 16: 4,\n",
       " 17: 2,\n",
       " 18: 2,\n",
       " 20: 1,\n",
       " 21: 1,\n",
       " 22: 1,\n",
       " 23: 1,\n",
       " 24: 1,\n",
       " 25: 2,\n",
       " 26: 1,\n",
       " 29: 1,\n",
       " 32: 1,\n",
       " 33: 2,\n",
       " 41: 1,\n",
       " 48: 1,\n",
       " 49: 1}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_distribution('variables/clusters_0.97.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the threshold further above 0.95 will make the largest clusters even smaller. This is only required if the sentences in the cluster with 0.95 as similarity threshold don't seem similar in reality. We now have to check if the clustering was performed well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will reload the cluster based on the 0.95 similarity threshold and print out the contents of the largest clusters. Remember that we have performed the clustering on the core of the sentences; the simplified sentences. Therefore, we still need to map the cluster id to the original sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_check_cluster_contents(cluster_filename, all_sentences, top_x_clusters=20):\n",
    "    with open(cluster_filename, 'rb') as infile:\n",
    "        clusters = pickle.load(infile)\n",
    "    # sort based on size\n",
    "    sorted_clusters = sorted(clusters, key=lambda x: len(x), reverse=True)\n",
    "\n",
    "    # create map between id and sentence\n",
    "    id_to_sentence = dict()\n",
    "    for entry in all_sentences:\n",
    "        id_to_sentence[entry[0]] = entry[1]\n",
    "    \n",
    "    # print contents of the clusters\n",
    "    for cluster_nr, cluster in enumerate(sorted_clusters[0:top_x_clusters]):\n",
    "        print('\\n\\n CLUSTER', cluster_nr+1)\n",
    "        for i in cluster:\n",
    "            print(id_to_sentence[i])\n",
    "    \n",
    "    return sorted_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the function to print the composition of the biggest clusters and check if the grouped sentences actually are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " CLUSTER 1\n",
      "Okay, maybe, maybe right in here.\n",
      "Maybe there's, there it is, there it is.\n",
      "Okay maybe, maybe.\n",
      "Now maybe, maybe, there we go.\n",
      "Okay, maybe right there, right there.\n",
      "Maybe there's some out in here.\n",
      "Maybe there it is.\n",
      "And maybe, maybe.\n",
      "Right about there, and maybe there.\n",
      "Maybe, maybe.\n",
      "Maybe right up through here.\n",
      "Maybe, right there, yep.\n",
      "Maybe, maybe, there he is.\n",
      "Okay, maybe right in here.\n",
      "Maybe maybe maybe maybe, yep.\n",
      "Maybe there's a... Right here, there it is.\n",
      "Maybe just right there.\n",
      "Okay, now, maybe, maybe, maybe, maybe, right there.\n",
      "Maybe right out on here.\n",
      "And maybe over here.\n",
      "And maybe up in here, there we go.\n",
      "Let's go in here maybe.\n",
      "Okay, maybe right down through here.\n",
      "Maybe right over in here too.\n",
      "Maybe, maybe, maybe, you can.\n",
      "And maybe.\n",
      "And maybe up here.\n",
      "Maybe right here.\n",
      "Maybe you are.\n",
      "Maybe, there it is.\n",
      "Maybe we'll have some over here.\n",
      "Maybe, right in here.\n",
      "Maybe, maybe, maybe, right there.\n",
      "Maybe, maybe, there it is.\n",
      "Okay, maybe, maybe, maybe, there's one right there.\n",
      "And maybe, maybe, maybe there's one right here.\n",
      "Just here and there, and maybe this.\n",
      "Maybe over here.\n",
      "Okay, maybe there's a few right in here.\n",
      "Maybe there's, there it is.\n",
      "And maybe, maybe, maybe right here.\n",
      "And maybe, maybe right here.\n",
      "Let's go over here, maybe.\n",
      "Maybe there some here.\n",
      "Okay, maybe, maybe, maybe, maybe.\n",
      "Maybe there's some over here in these.\n",
      "Kay, maybe, maybe, maybe, right in here.\n",
      "Maybe, maybe, maybe, maybe right here.\n",
      "And maybe, maybe, there it is.\n",
      "Okay, maybe right there.\n",
      "And maybe some over here.\n",
      "Okay, let's go maybe, maybe, maybe, there it is.\n",
      "Maybe, maybe, maybe.\n",
      "Maybe out through here.\n",
      "Maybe, right there.\n",
      "Okay, maybe.\n",
      "Maybe, yep, you're right, you're right.\n",
      "Maybe right there.\n",
      "Maybe, maybe, maybe, maybe, maybe, maybe, yep, you're right, you were right there's another one.\n",
      "Okay, let's go up here and maybe, maybe, maybe... There it is.\n",
      "And maybe right there, right there.\n",
      "Maybe there's, there he is.\n",
      "Maybe, maybe, right here.\n",
      "Maybe, maybe right on down through here.\n",
      "Maybe, maybe, yep there it is.\n",
      "Maybe from here right down.\n",
      "And maybe... There it is.\n",
      "Maybe there's a... Yep, there is.\n",
      "Okay maybe right out through there.\n",
      "And maybe, maybe, maybe, there.\n",
      "Maybe, maybe right there.\n",
      "Maybe there's some right up in here.\n",
      "Maybe, maybe, maybe, there it is.\n",
      "Maybe, maybe, maybe, maybe, maybe, there it is.\n",
      "Maybe, maybe, maybe, maybe, let's go right here.\n",
      "And maybe, maybe, maybe, maybe, there it is.\n",
      "Maybe over in here.\n",
      "Maybe, maybe, there it is, right there.\n",
      "Maybe right in here.\n",
      "There, okay maybe, maybe.\n",
      "Maybe there's a few in here.\n",
      "And maybe, there it is right there.\n",
      "Maybe, maybe, maybe right over here.\n",
      "And maybe ... You're right, there it is.\n",
      "Okay, maybe, there it is.\n",
      "Maybe some here and there.\n",
      "Maybe.\n",
      "\n",
      "\n",
      " CLUSTER 2\n",
      "So, there we go.\n",
      "There we go, there we go.\n",
      "And go from there.\n",
      "Okay let's go up here.\n",
      "And go.\n",
      "So just go out.\n",
      "Umkay now we can go in here.\n",
      "We'll just go out here.\n",
      "Okay, there we go.\n",
      "So there we go.\n",
      "Okay, so there we go.\n",
      "There we go, okay, let's go up here.\n",
      "Let's go.\n",
      "There they go.\n",
      "Okay, now, there we go.\n",
      "There we go, okay.\n",
      "You can just go up here.\n",
      "There we go, there's some more.\n",
      "There we go, now then.\n",
      "Let's go into some ocher.\n",
      "And, there we go.\n",
      "Okay, let's go over here.\n",
      "Go on.\n",
      "Here we go.\n",
      "Just sorta, there we go.\n",
      "There you go.\n",
      "Okay, here we go.\n",
      "there we go.\n",
      "And there we go.\n",
      "So, let's go.\n",
      "Go.\n",
      "Let's go up here.\n",
      "Okay, let's go up here.\n",
      "Where does this go?\n",
      "So, let's go here.\n",
      "So, let's go up here and do it.\n",
      "Where does it go?\n",
      "'Kay, there we go.\n",
      "You should just go.\n",
      "Go, go, go!\n",
      "Okay, and then we'll go up here.\n",
      "Let's go over here.\n",
      "Here you go.\n",
      "There we go.\n",
      "Okay, and there we go.\n",
      "There you go, okay let's go up here.\n",
      "That's where they go.\n",
      "There we go, now.\n",
      "There, there we go.\n",
      "\n",
      "\n",
      " CLUSTER 3\n",
      "That sharpens it right up.\n",
      "Right on up.\n",
      "Okay, right there.\n",
      "Just right on out.\n",
      "Right in here, a few.\n",
      "Right about in here.\n",
      "Right in here.\n",
      "Right here, there it is.\n",
      "Right here.\n",
      "And that's about where I'm at right now.\n",
      "All right, all right, all right.\n",
      "You were right.\n",
      "Now, right down here.\n",
      "All right now.\n",
      "All right.\n",
      "Right in through there.\n",
      "Right there, there he is.\n",
      "You're right, there it is.\n",
      "Right there.\n",
      "Right on down.\n",
      "That's just right.\n",
      "Are you all right?\n",
      "Okay, now right down through here.\n",
      "It's all right here.\n",
      "There it is, right there.\n",
      "These cepes, they're right there.\n",
      "It's right there.\n",
      "Just right on up.\n",
      "Right there?\n",
      "All right, now then.\n",
      "Right down in here.\n",
      "Now, right in here.\n",
      "There's some right there.\n",
      "Right in there.\n",
      "Just right over.\n",
      "Right here, okay.\n",
      "Just right in through there.\n",
      "Just right here.\n",
      "Right on out.\n",
      "Okay, there's a right there.\n",
      "Right.\n",
      "Right there, right there.\n",
      "Right over here.\n",
      "Here's some right there.\n",
      "Just right in here.\n",
      "Now, right down in here.\n",
      "Some right out here.\n",
      "Right down through here.\n",
      "And right, right here, right there.\n",
      "\n",
      "\n",
      " CLUSTER 4\n",
      "A little bit.\n",
      "Now, a little bit more.\n",
      "Tiniest little bit.\n",
      "Little bit up here.\n",
      "A little tiny bit.\n",
      "A little bit in here.\n",
      "Just do a little bit of that.\n",
      "Just a little bit there.\n",
      "Fluff it a little bit.\n",
      "Little bit over there.\n",
      "Maybe a little tiny bit in here.\n",
      "A little bit over here.\n",
      "And a little bit over in here.\n",
      "Just a little bit.\n",
      "Just to sparkle it up a little bit.\n",
      "A little bit more of the paintthinner.\n",
      "Just a little bit here and there.\n",
      "Maybe, a little bit there.\n",
      "Tiny little bit.\n",
      "Little bit over in here.\n",
      "Little bit for over here.\n",
      "Little bit, little bit.\n",
      "We'll fluff it out a little bit.\n",
      "Little bit in here.\n",
      "Maybe a little bit up in here.\n",
      "Little bit over here.\n",
      "And maybe, maybe over here, maybe there's a, there, just a little bit on that limb.\n",
      "And a little bit.\n",
      "Okay, a little bit up in here.\n",
      "A little bit here, maybe, maybe.\n",
      "Just here and there a little bit.\n",
      "Tiniest, tineist little bit.\n",
      "Just a little bit, little bit, it's very, very strong.\n",
      "Little bit up in here.\n",
      "Into a little bit of ocre.\n",
      "Just a little bit, just to sparkle him a little.\n",
      "And then we'll fluff it up a little bit.\n",
      "Tiny, tiniest little bit.\n",
      "Little little bit.\n",
      "Fluff it up a little bit.\n",
      "Okay, a little bit here.\n",
      "Maybe a little bit over in here.\n",
      "A tiny little bit.\n",
      "Just a tiny little bit.\n",
      "Okay, a little bit over here.\n",
      "\n",
      "\n",
      " CLUSTER 5\n",
      "Let it go there.\n",
      "And we'll just let these happen.\n",
      "Just let them go.\n",
      "Let them go, let them happen.\n",
      "Let them go, here and there.\n",
      "And just let them go.\n",
      "Let it go, let it go.\n",
      "Just let 'em happen.\n",
      "Just let 'em go.\n",
      "Just let 'em happen, let 'em happen.\n",
      "Just let it go there.\n",
      "Just let them happen, let it happen.\n",
      "Let it happen.\n",
      "Let him go.\n",
      "And we just let him go.\n",
      "And just let this go.\n",
      "And then let them go.\n",
      "Let them happen.\n",
      "Let 'em all happen.\n",
      "Let it Let it happen.\n",
      "Just sorta let it happen.\n",
      "Just let these happen.\n",
      "And here, we let it happen.\n",
      "Just let 'em happen, here and there.\n",
      "Just let this happen.\n",
      "There, just let it go.\n",
      "Let them go.\n",
      "We just let it go.\n",
      "Just let it happen.\n",
      "Just let these go.\n",
      "Let 'em go.\n",
      "There it is, just let it go.\n",
      "Let 'em happen.\n",
      "Just let him go.\n",
      "And let it go.\n",
      "Just let them happen.\n",
      "Just let it go, let it happen, let it happen.\n",
      "Let it happen, let it happen.\n",
      "Let 'em go, let 'em go.\n",
      "Just let it go.\n",
      "Let it go.\n"
     ]
    }
   ],
   "source": [
    "sorted_clusters = load_and_check_cluster_contents('variables/clusters_0.95.pickle', all_sentences, top_x_clusters=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above illustrates very well, the clusters formed with about 95% similarity are quite good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we would not simplify the sentences first, the sentences would contain a lot more 'noise' due to stop words and punctuation marks. The clusters would not be as consistent in comparison to the ones based on the core of the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As final result from the above, we will create a dictionary where we number all clusters and put the full sentences in them. We no longer need the simplified sentences and the IDs. We will now map the full sentences to the cluster they belong in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map sentence ID's to full sentences\n",
    "id_to_full = dict()\n",
    "for ID, full_sentence, _ in all_sentences:\n",
    "    id_to_full[ID] = full_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map cluster number -> set with sentences\n",
    "cluster2sentence = dict()\n",
    "for cluster_nr, cluster in enumerate(sorted_clusters):\n",
    "    cluster2sentence[cluster_nr+1] = set()\n",
    "    for sentence_id in cluster:\n",
    "        # add full sentence\n",
    "        sentence = id_to_full[sentence_id]\n",
    "        cluster2sentence[cluster_nr+1].update([sentence])\n",
    "\n",
    "# map sentence -> cluster number\n",
    "sentence2cluster = dict()\n",
    "for k,v in cluster2sentence.items():\n",
    "    for w in v:\n",
    "        sentence2cluster[w] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this section, two dictionaries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"docs/sentence2cluster_cluster2sentence.png\" align=\"center\" width=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: although the sentences are depicted here as the ids (s1, s2, ...), in reality the full sentences are in the dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Important remarks\n",
    "Although the clustered sentences really did resemble each other, the large majority of sentences that were not clustered (15634 clusters of just one sentence out of 17077 clusters in total). We can attribute this to one of the following two causes (or both)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Human language, even when only considering painting instructions, is extremely variable. There exist a lot of sentences with an entirely different meaning.**\n",
    "\n",
    "Solution: \n",
    "\n",
    "- Gathering more data if possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The clustering was not optimal.**\n",
    "\n",
    "Solutions:\n",
    "\n",
    "- Consider implementing another clustering algorithm\n",
    "\n",
    "- Manually correct and merge more clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest problem is probably the first, the ambiguity and variability in human language. Gathering more data is a possibility and this can be done by downloading subtitles of the later seasons. However, the computation would be infeasible on your own computer and even with all seasons downloaded there would likely still not be enough data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will try to use the clusters as classes for a classification model. This way, we could predict the next sentence based on the preceding sentence and create our own painting instructions, Bob Ross style!\n",
    "\n",
    "There is one concern: the majority of the classes (clusters) contains just one sentence. This is extremely problematic, because we only have one observation (one sentence) that represents the class (the cluster). From a statistical viewpoint, we expect that this will result in a model with low accuracy. \n",
    "\n",
    "However, as this notebook is for teaching purposes (both for myself and to share methods with others), I will give it a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Bob Ross classification model (supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every sentence we have the cluster to which it belongs. Now, from these sentences and their position in the transcript, we will create features.\n",
    "\n",
    "Very often this is an iterative process. We start by trying one approach and based on the performance of the model we can engineer new features or remove others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.1 Get all words\n",
    "\n",
    "The first and most obvious approach to predict the next sentence based on the previous is to count which words are preceding and how many times we have observed each word. Therefore, we need a list of all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4235 unique words.\n"
     ]
    }
   ],
   "source": [
    "all_words = set()\n",
    "for sentence in unique_sentences:\n",
    "    # lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    for c in punctuation:\n",
    "        sentence = sentence.replace(c, '')\n",
    "    \n",
    "    # add to set if it's not a stop word\n",
    "    all_words.update([word for word in sentence.split()])\n",
    "\n",
    "\n",
    "# how many\n",
    "print('There are',len(all_words), 'unique words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these unique words, we will remove the misspelled words, stop words and words that contain numbers from the list. This way we try to avoid fitting on a single occurrence of a misspelled word instead of the 'right' context represented by the other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_in_word(word):\n",
    "    \"\"\"Return True if there is a number in the word.\"\"\"\n",
    "    for letter in word:\n",
    "        if letter in '0123456789':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3313 unique words after removing stop words and unrecognised words.\n",
      "Example of the remaining words: ['abandoned', 'abduction', 'ability', 'abode', 'absolute']\n"
     ]
    }
   ],
   "source": [
    "# remove stop words, not recognised words and words that contain numbers from the list\n",
    "all_stopwords=set(stopwords.words('english'))\n",
    "unrecognised_words = not_recognised_words(all_words)\n",
    "for word in all_words.copy():\n",
    "    if word in all_stopwords or word in unrecognised_words or number_in_word(word):\n",
    "        all_words.remove(word)\n",
    "\n",
    "all_words = sorted(all_words)\n",
    "# how many\n",
    "print('There are',len(all_words), 'unique words after removing stop words and unrecognised words.')\n",
    "print('Example of the remaining words:',all_words[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.2 Create features\n",
    "\n",
    "For every observed (~sentence) we now need to create a feature set based on these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we want the data entries to have the following format:\n",
    "\n",
    "<img src=\"docs/feature_table.png\">\n",
    "\n",
    "So for every sentence we want to count the preceding words and put this into the table together with the cluster to which the sentence belongs. This way, we can train the model using the words of the previous sentences to predict the cluster number. When we predict the cluster number, we can pick a sentence from the cluster to create our own painting instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will map the words to a column in the datatable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_column = dict()\n",
    "for index,word in enumerate(all_words):\n",
    "    word_to_column[word] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we will prepare the data table header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abandoned' 'abduction' 'ability' ..., 'zone' 'zoom' 'cluster_number']]\n"
     ]
    }
   ],
   "source": [
    "datatable_header = np.array(all_words)\n",
    "# the last column (index = -1) is going to be the cluster number\n",
    "datatable_header = np.append(datatable_header, ['cluster_number'])\n",
    "datatable_header = datatable_header.reshape((1,datatable_header.shape[0]))\n",
    "print(datatable_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will now loop over every sentence in the transcript and count the words in every transcript along the way. Run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatable = np.zeros((1,datatable_header.shape[1]))\n",
    "for transcript in parsed_transcripts:\n",
    "    # initialise the word count and reset it for every transcript\n",
    "    word_count = np.zeros(len(datatable[0]))\n",
    "    for line in transcript:\n",
    "        if line in sentence2cluster:\n",
    "            # add it as a row to the data table\n",
    "            \n",
    "            # extract all words from the row\n",
    "            words_in_line = extract_words([[line]])\n",
    "            # update the row\n",
    "            for word in words_in_line.keys():\n",
    "                if word in word_to_column:\n",
    "                    ix = word_to_column[word]\n",
    "                    # increase the count for that specific word by the count in the sentence\n",
    "                    word_count[ix] += words_in_line[word]\n",
    "            \n",
    "            # add cluster number and add the row to the datatable\n",
    "            new_row = word_count.copy()\n",
    "            new_row[-1] = sentence2cluster[line] # cluster number\n",
    "            datatable = np.append(datatable, [new_row], axis=0)\n",
    "datatable = datatable[1:]\n",
    "with open('variables/datatable.pickle', 'wb') as outfile:\n",
    "        pickle.dump(datatable, outfile)\n",
    "print(datatable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature matrix will be saved using pickle. This way we don't need to do the computation every time we work in the notebook.  We can load it back in using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('variables/datatable.pickle', 'rb') as infile:\n",
    "        datatable = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatable = np.vstack((datatable_header, datatable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abduction</th>\n",
       "      <th>ability</th>\n",
       "      <th>abode</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absorb</th>\n",
       "      <th>abstract</th>\n",
       "      <th>absurd</th>\n",
       "      <th>accents</th>\n",
       "      <th>...</th>\n",
       "      <th>you're</th>\n",
       "      <th>you've</th>\n",
       "      <th>young</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>cluster_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7536.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>463.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14486.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3314 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  abandoned abduction ability abode absolute absolutely absorb abstract  \\\n",
       "0       0.0       0.0     0.0   0.0      0.0        0.0    0.0      0.0   \n",
       "1       0.0       0.0     0.0   0.0      0.0        0.0    0.0      0.0   \n",
       "2       0.0       0.0     0.0   0.0      0.0        0.0    0.0      0.0   \n",
       "3       0.0       0.0     0.0   0.0      0.0        0.0    0.0      0.0   \n",
       "4       0.0       0.0     0.0   0.0      0.0        0.0    0.0      0.0   \n",
       "\n",
       "  absurd accents      ...       you're you've young  yup zero  zig  zip zone  \\\n",
       "0    0.0     0.0      ...          0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1    0.0     0.0      ...          0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2    0.0     0.0      ...          0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3    0.0     0.0      ...          0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4    0.0     0.0      ...          0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "  zoom cluster_number  \n",
       "0  0.0         7536.0  \n",
       "1  0.0         6300.0  \n",
       "2  0.0          463.0  \n",
       "3  0.0          602.0  \n",
       "4  0.0        14486.0  \n",
       "\n",
       "[5 rows x 3314 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.array([int(float(i)) for i in range(datatable.shape[0]-1)]).transpose()\n",
    "data = pd.DataFrame(data=datatable[1:], index=index, columns=datatable[0])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.3 Important remarks\n",
    "\n",
    "What we have done in creating the matrix above is essentially what is done by the 'CountVectorizer' function of sklearn.\n",
    "\n",
    "**There are some problems with creating a feature matrix in this type of NLP analysis:**\n",
    "- the majority of the values in the matrix will be zero\n",
    "- even though we reduced the amount of sentence classes by clustering, there are still way too much classes of sentences to ensure a good fit\n",
    "- some sentences are alone in their cluster (class with just one observation). these are problematic if we want to split into a training and test set (see further)\n",
    "\n",
    "These problems are quite common in this type of NLP analysis, with no clear solution available. One way to potentially improve the performance of the model is to create features in a different way.\n",
    "\n",
    "**Additional ways to create features:**\n",
    "- only save the words in the 5 preceding sentences instead of the entire transcript\n",
    "- weigh the words: make some words more impactful than others (see Tf–idf term weighting on sklearn)\n",
    "\n",
    "**Extra remarks**\n",
    "- For information, representing sentences by it's words is called the 'bag-of-words' representation.\n",
    "- Might be a possibility to improve the classification by stacking different classification algorithms on top of each other. For example passing the output of one classification algorithm as input for another classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Train and test split\n",
    "\n",
    "In machine learning, it is highly recommended to split the data into a train set, validation/development set and testing set. This allows us to estimate parameters using the validation/dev set and to test the accuracy independently on new entries using the test set. \n",
    "\n",
    "However, in this study, we will **not** split the data in a training set and a testing set. Usually, the testing set would represent how well the model generalizes to other (real-world) observations, in contrast to the training data. It measures the general prediction performance of the model and indirectly prevents overfitting to the training data. \n",
    "\n",
    "In this study, we have a lot of classes (clusters) that contain only one sentence. In other words, if a class with only one observation is contained in the test set, the training set would not even have an observation for this class and the model does not know that that particular class exists. The test accuracy would therefore be very low. Additionally, we are not concerned with overfitting as it's not a problem in the context of painting instructions.\n",
    "\n",
    "We will evaluate the model performance manually by checking the produced painting instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training purposes, we still need to split the class labels from the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = data['cluster_number']\n",
    "train_X = data.copy().drop('cluster_number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abduction</th>\n",
       "      <th>ability</th>\n",
       "      <th>abode</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absorb</th>\n",
       "      <th>abstract</th>\n",
       "      <th>absurd</th>\n",
       "      <th>accents</th>\n",
       "      <th>...</th>\n",
       "      <th>you'll</th>\n",
       "      <th>you're</th>\n",
       "      <th>you've</th>\n",
       "      <th>young</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3313 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  abandoned abduction ability abode absolute absolutely absorb abstract  \\\n",
       "0       0.0       0.0     0.0   0.0      0.0        0.0    0.0      0.0   \n",
       "1       0.0       0.0     0.0   0.0      0.0        0.0    0.0      0.0   \n",
       "2       0.0       0.0     0.0   0.0      0.0        0.0    0.0      0.0   \n",
       "3       0.0       0.0     0.0   0.0      0.0        0.0    0.0      0.0   \n",
       "4       0.0       0.0     0.0   0.0      0.0        0.0    0.0      0.0   \n",
       "\n",
       "  absurd accents ...  you'll you're you've young  yup zero  zig  zip zone zoom  \n",
       "0    0.0     0.0 ...     0.0    0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1    0.0     0.0 ...     0.0    0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2    0.0     0.0 ...     0.0    0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3    0.0     0.0 ...     0.0    0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    0.0     0.0 ...     0.0    0.0    0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 3313 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     7536.0\n",
       "1     6300.0\n",
       "2      463.0\n",
       "3      602.0\n",
       "4    14486.0\n",
       "Name: cluster_number, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Fitting the multi-class classification model\n",
    "\n",
    "A collection of multi-class classification models can be found on:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/multiclass.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by checking some out of the box sklearn classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RandomForest_10 algorithm scored 0.8501342653765824 on the training dataset.\n",
      "The KNearestNeighbors_5 algorithm scored 0.2207067047440433 on the training dataset.\n",
      "The DecisionTree algorithm scored 0.8835940496995013 on the training dataset.\n",
      "CPU times: user 5min 34s, sys: 1min 56s, total: 7min 30s\n",
      "Wall time: 8min 10s\n"
     ]
    }
   ],
   "source": [
    "sklearn_classifiers_dict = {'KNearestNeighbors_5': sklearn.neighbors.KNeighborsClassifier(n_neighbors=5),\n",
    "                            'DecisionTree': DecisionTreeClassifier(),\n",
    "                            'RandomForest_10': RandomForestClassifier(n_estimators=10)}\n",
    "\n",
    "for algo in sklearn_classifiers_dict.keys():\n",
    "    clf = sklearn_classifiers_dict[algo]\n",
    "    clf.fit(train_X, train_y)\n",
    "    print('The {} algorithm scored {} on the training dataset.'.format(algo, clf.score(train_X, train_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following take a lot longer to fit. Run them if you want, but we will continue with the faster algorithms from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sklearn_classifiers_dict = {'LogisticRegression_ovr': sklearn.linear_model.LogisticRegression()}\n",
    "\n",
    "for algo in sklearn_classifiers_dict.keys():\n",
    "    clf = sklearn_classifiers_dict[algo]\n",
    "    clf.fit(train_X, train_y)\n",
    "    print('The {} algorithm scored {} on the training dataset.'.format(algo, clf.score(train_X, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sklearn_classifiers_dict = {'LinearSVC_ovr': sklearn.svm.LinearSVC(multi_class='ovr'), \n",
    " 'LinearSVC_cs': sklearn.svm.LinearSVC(multi_class='crammer_singer')}\n",
    "\n",
    "for algo in sklearn_classifiers_dict.keys():\n",
    "    clf = sklearn_classifiers_dict[algo]\n",
    "    clf.fit(train_X, train_y)\n",
    "    print('The {} algorithm scored {} on the training dataset.'.format(algo, clf.score(train_X, train_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue with the Random Forest Classifier, which is a collection of decision trees. We will play around with the parameter 'n_estimators' to find an optimal amount of trees in the forest. This step is usually done on the validation set, but for the reasons mentioned above we will do it on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trees, scores = list(), list()\n",
    "for n in np.arange(5,20,2):\n",
    "    clf = RandomForestClassifier(n_estimators=n)\n",
    "    clf.fit(train_X, train_y)\n",
    "    score = clf.score(train_X, train_y)\n",
    "    trees.append(n)\n",
    "    scores.append(score)\n",
    "    print('n_estimators = {}, training set accuracy = {}'.format(n, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python kernel on my machine dies when fitting with n_estimators > 13. So we will end the computation there."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_estimators = 5, training set accuracy = 0.7754145177102425\n",
    "n_estimators = 7, training set accuracy = 0.8162482417629257\n",
    "n_estimators = 9, training set accuracy = 0.8402881377605388\n",
    "n_estimators = 11, training set accuracy = 0.8555901282980265\n",
    "n_estimators = 13, training set accuracy = 0.8644985294744469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGHCAYAAABrpPKuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110577400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "trees = [5,7,9,11,13]\n",
    "scores = [0.775, 0.816, 0.840, 0.856, 0.864]\n",
    "plt.scatter(trees, scores)\n",
    "plt.title('Accuracy vs. n_estimators')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue with the RandomForestClassifier with n_estimators = 13. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.865777247347\n"
     ]
    }
   ],
   "source": [
    "BobRoss = RandomForestClassifier(n_estimators=13)\n",
    "BobRoss.fit(train_X, train_y)\n",
    "print('Training accuracy:',BobRoss.score(train_X, train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 Generating our own painting instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict the next sentences based on the previous, we first need to write at least one starting sentence ourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_transcript = ['I would like to paint mountains, water and happy little trees.']\n",
    "\n",
    "# the number of sentences you want the transcript to contain\n",
    "max_transcript_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Initialise feature list, classification model and maximum transcript length\n",
    "\n",
    "-  Extract the words\n",
    "\n",
    "-  Update feature list\n",
    "\n",
    "-  Predict next sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise feature list and model\n",
    "features = np.zeros((1, train_X.shape[1]))\n",
    "BobRoss = RandomForestClassifier(n_estimators=13)\n",
    "BobRoss.fit(train_X, train_y)\n",
    "\n",
    "new_train_X, new_train_y = train_X, train_y\n",
    "\n",
    "while len(new_transcript) < max_transcript_length:\n",
    "    # 1) extract words from last sentence\n",
    "    last_sentence = new_transcript[-1]\n",
    "    words = extract_words([[last_sentence]])\n",
    "    \n",
    "    # 2) Update feature list\n",
    "    for word in words.keys():\n",
    "        if word in word_to_column:\n",
    "            features[0][word_to_column[word]] += int(words[word])\n",
    "    \n",
    "    # 3) predict next sentence\n",
    "    next_sentence_cluster = BobRoss.predict(features)\n",
    "    sentences_in_cluster = cluster2sentence[float(next_sentence_cluster[0])]\n",
    "    while len(sentences_in_cluster) == 0:\n",
    "        # refit model without that cluster if all sentences in the cluster are used up\n",
    "        cluster_ix = new_train_y != str(next_sentence_cluster)\n",
    "        new_train_X, new_train_y = new_train_X[cluster_ix], new_train_y[cluster_ix] \n",
    "        BobRoss.fit(new_train_X, new_train_y)\n",
    "        next_sentence_cluster = BobRoss.predict(features)\n",
    "        sentences_in_cluster = cluster2sentence[float(next_sentence_cluster[0])]\n",
    "        \n",
    "    next_sentence = sentences_in_cluster.pop()\n",
    "    new_transcript.append(next_sentence)\n",
    "    print(next_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you've been with us before, I hope you have your almighty easel set up, and you're all ready to go along with me and paint a fantastic little painting.\n",
    "\n",
    "> Hope you have your easel set up and you're ready to paint along with me this day.\n",
    "\n",
    "> And today, I hope you've got your palette set up, and a big glass of iced tea there, and you're ready to paint along with me.\n",
    "\n",
    "> You got your palette ready, your easel set up, big glass of tea right beside you?\n",
    "\n",
    "> Thought today we'd show you how to do snow.\n",
    "\n",
    "> How do you make it look like the snow is falling?\n",
    "\n",
    "> So I thought that's what we'd do today.\n",
    "\n",
    "> Today I thought we'd do it with this.\n",
    "\n",
    "> Today I thought we'd do a little painting that's just a lot of fun and I really think you'll enjoy it.\n",
    "\n",
    "> I've already covered the canvas with a thin, even coat of magic white, so it's wet, slick, and ready to go.\n",
    "\n",
    "> I've already covered the canvas with a thin even coat of magic white, and it's wet and slick and ready to go.\n",
    "\n",
    "> Now I've already covered this canvas with a thin, even coat of Magic White so it's nice and wet and slick and ready to go.\n",
    "\n",
    "> I've already covered the canvas with a thin, even coat of Magic White, so it's wet and ready to go.\n",
    "\n",
    "> I've already covered the canvas with a thin even coat of Magic White and we're ready to go.\n",
    "\n",
    "> So, I'm gonna start here with a little bit of Prussian blue.\n",
    "\n",
    "> I'm gonna start off here today with a little bit of Prussian blue.\n",
    "\n",
    "> That'll set it off, even a little bit more off into the distance.\n",
    "\n",
    "> I thought today we'd do a very quick happy little waterfall.\n",
    "\n",
    "> Alright, we're going to run the colors across the screen in case you didn't get them last week, and just let you see all the colors that we use on each series here.\n",
    "\n",
    "> And it'll pay you to wipe the brush in between because you're gonna pick up a little bit of that blue.\n",
    "\n",
    "> Just drop them in real quick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately see that there is a lot of repetition in the transcript. We tweak the script by making sure the model refits without the already used clusters to avoid repetition.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise feature list and model\n",
    "features = np.zeros((1, train_X.shape[1]))\n",
    "BobRoss = RandomForestClassifier(n_estimators=13)\n",
    "BobRoss.fit(train_X, train_y)\n",
    "\n",
    "new_train_X, new_train_y = train_X.copy(), train_y.copy()\n",
    "\n",
    "while len(new_transcript) < max_transcript_length:\n",
    "    # 1) extract words from last sentence\n",
    "    last_sentence = new_transcript[-1]\n",
    "    words = extract_words([[last_sentence]])\n",
    "    \n",
    "    # 2) Update feature list\n",
    "    for word in words.keys():\n",
    "        if word in word_to_column:\n",
    "            features[0][word_to_column[word]] += int(words[word])\n",
    "    \n",
    "    # 3) predict next sentence\n",
    "    next_sentence_cluster = BobRoss.predict(features)\n",
    "    sentences_in_cluster = cluster2sentence[float(next_sentence_cluster[0])]\n",
    "\n",
    "    \n",
    "    # refit model without that cluster\n",
    "    cluster_ix = new_train_y != str(next_sentence_cluster[0])\n",
    "    new_train_X, new_train_y = new_train_X[cluster_ix], new_train_y[cluster_ix] \n",
    "    BobRoss.fit(new_train_X, new_train_y)\n",
    "    \n",
    "    next_sentence = sentences_in_cluster.pop()\n",
    "    new_transcript.append(next_sentence)\n",
    "    print(next_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result:\n",
    "\n",
    "You don't have to read through all of it, you can find the remarks below the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> START: I would like to paint **mountains**, **water** and happy little **trees**.\n",
    "\n",
    "> Hi.\n",
    "\n",
    "> Son of a gun.\n",
    "\n",
    "> And, some on this side.\n",
    "\n",
    "> Arrey, Hassan.\n",
    "\n",
    "> Hello, I'm Bob Ross and I'd like to welcome you to the fourth Joy of Painting series.\n",
    "\n",
    "> For the next 13 shows, I'll be your host as we explore this fantastic method of painting and if this is your first time with us, I think you'll find this a very exciting method of oil painting.\n",
    "\n",
    "> We're gonna take here and use some very simple tools, a dozen colors, and I'm gonna show you how to do fantastic things.\n",
    "\n",
    "> So let's start out here, and I'm gonna cover the entire canvas with a thin, even coat of magic white.\n",
    "\n",
    "> Now magic white is a liquid paint that makes the canvas wet and makes it slick and allows things to happen right here on the canvas.\n",
    "\n",
    "> We allow color to blend on the canvas rather than worry about mixing it on the canvas.\n",
    "\n",
    "> There we go, there's some more.\n",
    "\n",
    "> Use long horizontal and vertical strokes to make sure you have a nice, even distribution of magic white across the canvas.\n",
    "\n",
    "> And we're ready to go.\n",
    "\n",
    "> Today I'm gonna use a very very limited palette, we're only going to use about four colors and I'm gonna have them graphically run those colors across the screen so you can pick them up at home.\n",
    "\n",
    "> These colors will come across your screen in the same order that I have them on my palette, starting with the white and working across.\n",
    "\n",
    "> Let me wash the brush and we wash our brushes with odorless paint thinner and I have a screen in the bottom of my can that I scrub the brush against to remove the paint.\n",
    "\n",
    "> Shake off the excess There we go, that's the most fun part of it.\n",
    "\n",
    "> Okay today, let's mix up a purple color and I'll take alizarin crimson, we might as well mix up a pretty good batch, we're going to be using it all through this little painting and thalo blue.\n",
    "\n",
    "> Now the blue is many, many times stronger than the alizarin crimson so use very little, very little in comparison to the crimson.\n",
    "\n",
    "> Okay, pick the paint up off the palette and turn it over when you're mixing it, that assures it is mixed very thorough.\n",
    "\n",
    "> There, and we'll mix this pretty thoroughly.\n",
    "\n",
    "> Now it's very hard to tell what color you have here, it just looks black so let me wipe off the knife and we'll take a little bit of white and then we'll touch that and we'll check it, we'll see what we have here and if that's the color that we're looking for and that's pretty good color so we'll use it.\n",
    "\n",
    "> So now, with a large two and a half inch brush, I'm gonna take just a little bit of the color on the brush and really work the paint into the bristles, work it into the bristles so you have a nice, even distribution of color.\n",
    "\n",
    "> Now then, let's go to the canvas maybe right in here.\n",
    "\n",
    "> I'm gonna start right at the top of the canvas and make little criss cross strokes, little x's.\n",
    "\n",
    "> It's easy to make a sky darker but it's a son of a gun to make it lighter so use very little paint, you can always add more.\n",
    "\n",
    "> Go all the way across the top using the little criss cross strokes and begin blending downward.\n",
    "\n",
    "> Now your color is mixing with the magic white and automatically, it gets lighter as it goes toward the horizon and that's what we're looking for.\n",
    "\n",
    "> As things get closer to you in the landscape, it should get darker in value.\n",
    "\n",
    "> Okay, we'll take a little more color, just go right up here, still making the little criss cross strokes and blend downward so that it gets much, much lighter right here at the horizon.\n",
    "\n",
    "> Very gently we'll blend that.\n",
    "\n",
    "> Now while I've got the color on the brush, I'm gonna load a little bit more and we'll have some **water** down here at the bottom.\n",
    "\n",
    "> One thing about water, if it's still water, it's always flat so pull from the outside in, the outside in and keep these strokes as straight as possible, it'll make your water flat, make it lay down.\n",
    "\n",
    "> Very gently just blend it all together.\n",
    "\n",
    "> This is three hairs and some air, just barely caressing the canvas.\n",
    "\n",
    "> Okay, now we'll come from the other side and do the same thing.\n",
    "\n",
    "> As I say, if this is your first time joining us, you'll notice that we do not do any tracings, we use no patterns, we just sort of let these paintings flow right out of our heart and you can too.\n",
    "\n",
    "> so it makes it fantastic.\n",
    "\n",
    "> Let's clean our brush off.\n",
    "\n",
    "> Once again, we use odorless paint thinner.\n",
    "\n",
    "> Alrighty, let's use the fan brush some today and I'm gonna go right back in here and I'm gonna use the same dark color and figure out, maybe there's a **tree** line that lives right there, there it is.\n",
    "\n",
    "> And all I'm doing here is touching the canvas and pulling down.\n",
    "\n",
    "> Touch and pull down, I'll try to hold the brush sideways so you can see that close up.\n",
    "\n",
    "> Touch and pull down, okay?\n",
    "\n",
    "> And we just sort of let this wander right on back and we're still using the same purple color, lavender color, purple.\n",
    "\n",
    "> And just let 'em fade right out to nothing.\n",
    "\n",
    "> Okay, now let's bring this little **hill**, I know this is gonna be a little hill right here, I see a little hill.\n",
    "\n",
    "> Bring it down just like so and then lift gently upward, I'm just touching and lifting straight up, straight up.\n",
    "\n",
    "> Good, now I want to create the illusion of mist so we'll take the large brush and, very gently, I'm gonna tap the bottom of this, just tapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with a sentence that contained the words **mountain**, **water** and **trees**. We therefore also expect that because the starting features have non-zero values for mountain, water and trees, we will have a transcript that includes these shapes of nature.\n",
    "\n",
    "The result is a transcript that starts off in a weird way (especially the first 4 sentences), but comes together as the transcript becomes longer. It looks quite coherent and shows instructions containing **water**, **trees** and a **hill** in this order.\n",
    "\n",
    "However, if you would actually paint following these instructions, you will probably have a very bad time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.5 Evaluation and final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clustering**\n",
    "\n",
    "As discussed before, the clustering step can probably be improved. Too many sentences are not clustered together, which is expected, but a manual correction is recommended.\n",
    "\n",
    "**feature engineering**\n",
    "\n",
    "We should definitely explore the possibilities by designing other features and making combinations of existing ones. This could really improve the performance of the model a lot. This step is extremely crucial. \n",
    "\n",
    "**train test split**\n",
    "\n",
    "The train test split remains a very hard problem because the data is very limited. We also need to come up with a metric that evaluates the quality of the entire transcript and how well it actually guides us while painting. A lot time can be spent here. \n",
    "\n",
    "**the model**\n",
    "\n",
    "The choice was made to go with a random forest without evaluating many other possibilities. If we wish to increase the performance, we need to evaluate different models. Also, for determining optimal parameter values, it's a good idea to check different settings on a validation set. However, the computational recourses available for this intensive task were not available.\n",
    "\n",
    "**evaluating the transcript**\n",
    "\n",
    "I manually read through and evaluated one transcript. As mentioned above, a satisfying evaluation metric should be created and more transcripts should be generated to get a read on the general performance of our Bob Ross model. \n",
    "\n",
    "**general remark**\n",
    "\n",
    "NLP is a very interesting field, but also very hard to model mathematically due to the variability and the ambiguity of the human language. Hopefully the future brings us smarter and more straight forward approaches on how to tackle a problem of this size and complexity. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "225px",
    "width": "223px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "40px",
    "left": "31.9886px",
    "right": "20px",
    "top": "105.781px",
    "width": "425px"
   },
   "toc_section_display": "none",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
